#!/usr/bin/env python3
"""
–†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–ê–Ø –ü–ï–†–ï–ò–ù–î–ï–ö–°–ê–¶–ò–Ø: –£–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –¥–ª—è –≤—Å–µ—Ö PDF
–≠—Ç–∞ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ä–µ—à–∏—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ RAG
"""

import sys
import os
from pathlib import Path
import time
from tqdm import tqdm

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
sys.path.insert(0, str(Path(__file__).parent / "src"))
sys.path.insert(0, str(Path(__file__).parent))

from lysobacter_rag.indexer.indexer import Indexer
from lysobacter_rag.pdf_extractor.advanced_pdf_extractor import AdvancedPDFExtractor
from config import config

def reindex_with_smart_chunking():
    """–ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å —É–º–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º"""
    
    print("üß¨ –†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–ê–Ø –ü–ï–†–ï–ò–ù–î–ï–ö–°–ê–¶–ò–Ø –° –£–ú–ù–´–ú –ß–ê–ù–ö–ò–ù–ì–û–ú")
    print("=" * 70)
    print("üí° –¶–µ–ª—å: –°–æ–∑–¥–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ RAG")
    print()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ
    data_dir = Path("data")
    if not data_dir.exists():
        print("‚ùå –ü–∞–ø–∫–∞ data/ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        return False
    
    pdf_files = sorted(list(data_dir.glob("*.pdf")))
    if not pdf_files:
        print("‚ùå PDF —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–∞–ø–∫–µ data/!")
        return False
    
    print(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(pdf_files)} PDF —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    print("\nüöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã...")
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å —É–º–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º
    extractor = AdvancedPDFExtractor(use_smart_chunking=True)
    print("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å —É–º–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º –≥–æ—Ç–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å–µ—Ä
    indexer = Indexer()
    print("‚úÖ –ò–Ω–¥–µ–∫—Å–µ—Ä –≥–æ—Ç–æ–≤")
    
    # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
    print("\nüóëÔ∏è –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    try:
        indexer.clear_collection()
        print("‚úÖ –°—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã")
    except Exception as e:
        print(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ: {e}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_chunks = 0
    total_documents = 0
    total_errors = 0
    processing_stats = {
        'successful_files': [],
        'failed_files': [],
        'chunking_stats': {
            'total_elements': 0,
            'total_chunks': 0,
            'avg_chunk_size': 0,
            'critical_chunks': 0,
            'high_importance_chunks': 0
        }
    }
    
    start_time = time.time()
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã
    print(f"\nüìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF —Ñ–∞–π–ª–æ–≤:")
    print("-" * 50)
    
    for i, pdf_file in enumerate(tqdm(pdf_files, desc="–ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è"), 1):
        try:
            print(f"\n{i:2d}/{len(pdf_files)} üìñ {pdf_file.name}")
            
            # –®–∞–≥ 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–º
            document = extractor.extract_document(pdf_file)
            
            if not document.elements:
                print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —ç–ª–µ–º–µ–Ω—Ç—ã")
                processing_stats['failed_files'].append({
                    'file': pdf_file.name,
                    'reason': 'no_elements_extracted'
                })
                continue
            
            print(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(document.elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            processing_stats['chunking_stats']['total_elements'] += len(document.elements)
            
            # –®–∞–≥ 2: –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —É–º–Ω–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞
            chunks = extractor.get_smart_chunks(document)
            
            if not chunks:
                print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏")
                processing_stats['failed_files'].append({
                    'file': pdf_file.name,
                    'reason': 'no_chunks_created'
                })
                continue
            
            print(f"   üß¨ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —É–º–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
            chunk_sizes = [len(chunk['content']) for chunk in chunks]
            avg_chunk_size = sum(chunk_sizes) / len(chunk_sizes) if chunk_sizes else 0
            
            importance_counts = {}
            for chunk in chunks:
                importance = chunk['metadata'].get('scientific_importance', 'unknown')
                importance_counts[importance] = importance_counts.get(importance, 0) + 1
            
            print(f"   üìä –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {avg_chunk_size:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   üéØ –í–∞–∂–Ω–æ—Å—Ç—å: {importance_counts}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            processing_stats['chunking_stats']['total_chunks'] += len(chunks)
            processing_stats['chunking_stats']['critical_chunks'] += importance_counts.get('critical', 0)
            processing_stats['chunking_stats']['high_importance_chunks'] += importance_counts.get('high', 0)
            
            # –®–∞–≥ 3: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DocumentChunk –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            from lysobacter_rag.data_processor import DocumentChunk
            
            document_chunks = []
            for i, chunk in enumerate(chunks):
                doc_chunk = DocumentChunk(
                    chunk_id=f"{pdf_file.stem}_{i}",
                    text=chunk['content'],
                    chunk_type=chunk['metadata'].get('chunk_type', 'text'),
                    metadata=chunk['metadata']
                )
                document_chunks.append(doc_chunk)
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –±–∞—Ç—á–µ–º
            try:
                success = indexer.index_chunks(document_chunks)
                if success:
                    successful_chunks = len(document_chunks)
                    print(f"   üíæ –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {successful_chunks} —á–∞–Ω–∫–æ–≤")
                else:
                    successful_chunks = 0
                    print(f"   ‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –±–∞—Ç—á–∞")
            except Exception as e:
                successful_chunks = 0
                print(f"   ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
            
            total_chunks += successful_chunks
            total_documents += 1
            processing_stats['successful_files'].append({
                'file': pdf_file.name,
                'elements': len(document.elements),
                'chunks': len(chunks),
                'indexed_chunks': successful_chunks,
                'avg_chunk_size': avg_chunk_size,
                'importance_distribution': importance_counts
            })
            
        except Exception as e:
            print(f"   ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            total_errors += 1
            processing_stats['failed_files'].append({
                'file': pdf_file.name,
                'reason': f'critical_error: {str(e)}'
            })
            continue
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    end_time = time.time()
    processing_time = end_time - start_time
    
    print(f"\nüéâ –ü–ï–†–ï–ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
    print("=" * 50)
    
    print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time:.1f} —Å–µ–∫—É–Ω–¥")
    print(f"üìö –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {total_documents}/{len(pdf_files)}")
    print(f"üì¶ –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {total_chunks}")
    print(f"‚ùå –û—à–∏–±–æ–∫: {total_errors}")
    
    if total_chunks > 0:
        avg_system_chunk_size = processing_stats['chunking_stats']['total_chunks']
        if avg_system_chunk_size > 0:
            avg_system_chunk_size = processing_stats['chunking_stats']['total_elements'] / avg_system_chunk_size
        
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –£–ú–ù–û–ì–û –ß–ê–ù–ö–ò–ù–ì–ê:")
        print(f"   üìà –í—Å–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–æ: {processing_stats['chunking_stats']['total_elements']}")
        print(f"   üß¨ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {processing_stats['chunking_stats']['total_chunks']}")
        print(f"   üìè –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —á–∞–Ω–∫–∏–Ω–≥–∞: {processing_stats['chunking_stats']['total_chunks'] / processing_stats['chunking_stats']['total_elements']:.1f}x")
        print(f"   üéØ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {processing_stats['chunking_stats']['critical_chunks']}")
        print(f"   ‚≠ê –í—ã—Å–æ–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏: {processing_stats['chunking_stats']['high_importance_chunks']}")
        
        important_ratio = (processing_stats['chunking_stats']['critical_chunks'] + 
                          processing_stats['chunking_stats']['high_importance_chunks']) / processing_stats['chunking_stats']['total_chunks']
        print(f"   üí° –î–æ–ª—è –≤–∞–∂–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {important_ratio:.1%}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    print(f"\nüîç –ü–†–û–í–ï–†–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–ê:")
    
    final_stats = indexer.get_collection_stats()
    print(f"   üíæ –ß–∞–Ω–∫–æ–≤ –≤ –±–∞–∑–µ: {final_stats.get('total_chunks', 0)}")
    
    # –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞
    test_queries = [
        "Lysobacter capsici YC5194",
        "temperature range growth",
        "G+C content DNA",
        "type strain isolated"
    ]
    
    print(f"   üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞:")
    search_quality_scores = []
    
    for query in test_queries:
        try:
            results = indexer.search(query, top_k=3)
            if results:
                avg_relevance = sum(r['relevance_score'] for r in results) / len(results)
                search_quality_scores.append(avg_relevance)
                status = "‚úÖ" if avg_relevance > 0.4 else "‚ö†Ô∏è" if avg_relevance > 0.2 else "‚ùå"
                print(f"      {status} '{query}': {avg_relevance:.3f}")
            else:
                print(f"      ‚ùå '{query}': –ù–ï –ù–ê–ô–î–ï–ù")
                search_quality_scores.append(0.0)
        except Exception as e:
            print(f"      ‚ùå '{query}': –û–®–ò–ë–ö–ê {e}")
            search_quality_scores.append(0.0)
    
    avg_search_quality = sum(search_quality_scores) / len(search_quality_scores) if search_quality_scores else 0
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    print(f"\nüèÜ –ò–¢–û–ì–û–í–ê–Ø –û–¶–ï–ù–ö–ê:")
    print(f"   üìä –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞: {avg_search_quality:.3f}")
    print(f"   üìö –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_documents/len(pdf_files):.1%}")
    print(f"   üíæ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–µ: {final_stats.get('total_chunks', 0)}")
    
    if avg_search_quality >= 0.4 and total_documents >= len(pdf_files) * 0.8:
        print(f"   üéâ –£–°–ü–ï–•: –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
        success = True
    elif avg_search_quality >= 0.3 and total_documents >= len(pdf_files) * 0.6:
        print(f"   ‚úÖ –•–û–†–û–®–û: –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∏–µ–º–ª–µ–º–æ")
        success = True
    else:
        print(f"   ‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–´: –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞")
        success = False
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞...")
    
    report = {
        'timestamp': time.time(),
        'processing_time': processing_time,
        'total_files': len(pdf_files),
        'successful_files': total_documents,
        'failed_files': total_errors,
        'total_chunks': total_chunks,
        'avg_search_quality': avg_search_quality,
        'chunking_stats': processing_stats['chunking_stats'],
        'successful_files_details': processing_stats['successful_files'],
        'failed_files_details': processing_stats['failed_files']
    }
    
    import json
    with open('smart_chunking_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"   ‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: smart_chunking_report.json")
    
    return success

if __name__ == "__main__":
    try:
        success = reindex_with_smart_chunking()
        if success:
            print(f"\nüöÄ –ì–û–¢–û–í–û: –°–∏—Å—Ç–µ–º–∞ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∞ —Å —É–º–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º!")
            print(f"   –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —É–ª—É—á—à–µ–Ω–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤")
        else:
            print(f"\n‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏")
            print(f"   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏ –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å")
        
        exit(0 if success else 1)
        
    except KeyboardInterrupt:
        print(f"\n‚èπÔ∏è –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        exit(1)
    except Exception as e:
        print(f"\nüí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        exit(1) 