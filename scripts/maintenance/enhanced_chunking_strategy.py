#!/usr/bin/env python3
"""
–†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–ê–Ø –°–¢–†–ê–¢–ï–ì–ò–Ø: –£–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
"""

import re
import sys
from pathlib import Path
from typing import List, Dict, Tuple
import nltk
from sentence_transformers import SentenceTransformer

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
sys.path.insert(0, str(Path(__file__).parent / "src"))

class ScientificTextChunker:
    """–£–º–Ω—ã–π —á–∞–Ω–∫–µ—Ä –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –æ Lysobacter"""
    
    def __init__(self, target_chunk_size: int = 400, overlap: int = 50):
        self.target_chunk_size = target_chunk_size
        self.overlap = overlap
        
        # –ù–∞—É—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è Lysobacter
        self.section_patterns = [
            r"(Description|Morphology|Physiology|Biochemical characteristics)",
            r"(Growth conditions|Temperature range|pH range)",
            r"(Type strain|Strain|Isolate)",
            r"(G\+C content|DNA composition)",
            r"(16S rRNA|Phylogen|Taxonomy)",
            r"(Etymology|Diagnosis)"
        ]
        
        # –ö–ª—é—á–µ–≤—ã–µ –Ω–∞—É—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        self.key_terms = [
            "Lysobacter", "sp. nov.", "type strain", "isolate",
            "G+C content", "16S rRNA", "phylogenetic",
            "temperature", "pH", "growth", "morphology",
            "catalase", "oxidase", "gram-negative",
            "antibiotic", "antimicrobial", "biocontrol"
        ]
        
    def split_into_smart_chunks(self, text: str, source_pdf: str = "") -> List[Dict]:
        """–£–º–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏"""
        
        # 1. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        text = self._clean_text(text)
        
        # 2. –ü–æ–∏—Å–∫ —Å–µ–∫—Ü–∏–π
        sections = self._identify_sections(text)
        
        # 3. –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = self._split_sentences(text)
        
        # 4. –°–æ–∑–¥–∞–Ω–∏–µ —É–º–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        chunks = self._create_semantic_chunks(sentences, sections)
        
        # 5. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–æ–≤
        optimized_chunks = self._optimize_chunk_sizes(chunks)
        
        # 6. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        result = []
        for i, chunk in enumerate(optimized_chunks):
            result.append({
                'text': chunk['text'],
                'metadata': {
                    'source_pdf': source_pdf,
                    'chunk_type': chunk['type'],
                    'chunk_index': i,
                    'section': chunk.get('section', ''),
                    'key_terms': chunk.get('key_terms', []),
                    'scientific_importance': chunk.get('importance', 'medium')
                }
            })
            
        return result
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞—É—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
        text = re.sub(r'(\w+)-\s+(\w+)', r'\1\2', text)
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ —á–∏—Å–ª–∞ –∏ –µ–¥–∏–Ω–∏—Ü—ã
        text = re.sub(r'(\d+)\s*¬∞\s*C', r'\1¬∞C', text)
        text = re.sub(r'(\d+)\s*%', r'\1%', text)
        text = re.sub(r'pH\s+(\d+)', r'pH \1', text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞—É—á–Ω—ã–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
        text = re.sub(r'\bsp\.\s*nov\.\s*', 'sp. nov. ', text)
        text = re.sub(r'\bgen\.\s*nov\.\s*', 'gen. nov. ', text)
        
        return text.strip()
    
    def _identify_sections(self, text: str) -> List[Dict]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–µ–∫—Ü–∏–π –≤ —Ç–µ–∫—Å—Ç–µ"""
        sections = []
        
        for pattern in self.section_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                sections.append({
                    'start': match.start(),
                    'end': match.end(),
                    'type': match.group(1),
                    'importance': 'high'
                })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏
        sections.sort(key=lambda x: x['start'])
        return sections
    
    def _split_sentences(self, text: str) -> List[Dict]:
        """–†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –∞–Ω–∞–ª–∏–∑–æ–º –≤–∞–∂–Ω–æ—Å—Ç–∏"""
        
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        result = []
        for i, sentence in enumerate(sentences):
            if len(sentence.strip()) < 10:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ
                continue
                
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            importance = self._analyze_sentence_importance(sentence)
            key_terms = self._extract_key_terms(sentence)
            
            result.append({
                'text': sentence.strip(),
                'index': i,
                'importance': importance,
                'key_terms': key_terms,
                'length': len(sentence)
            })
            
        return result
    
    def _analyze_sentence_importance(self, sentence: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è"""
        
        # –í—ã—Å–æ–∫–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å
        high_indicators = [
            r'type strain', r'sp\. nov\.', r'isolate', r'strain.*isolated',
            r'G\+C content', r'DNA.*mol%', r'16S rRNA',
            r'temperature.*range', r'pH.*range', r'growth.*temperature',
            r'gram-negative', r'gram-positive', r'cell.*morphology',
            r'catalase.*positive', r'oxidase.*positive'
        ]
        
        # –°—Ä–µ–¥–Ω—è—è –≤–∞–∂–Ω–æ—Å—Ç—å  
        medium_indicators = [
            r'phylogenetic', r'sequence', r'similarity',
            r'antibiotic', r'antimicrobial', r'biocontrol',
            r'enzyme', r'activity', r'substrate'
        ]
        
        sentence_lower = sentence.lower()
        
        for pattern in high_indicators:
            if re.search(pattern, sentence_lower):
                return 'high'
        
        for pattern in medium_indicators:
            if re.search(pattern, sentence_lower):
                return 'medium'
        
        return 'low'
    
    def _extract_key_terms(self, sentence: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        found_terms = []
        
        sentence_lower = sentence.lower()
        for term in self.key_terms:
            if term.lower() in sentence_lower:
                found_terms.append(term)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        numbers = re.findall(r'\d+(?:\.\d+)?¬∞C|\d+(?:\.\d+)?%|pH\s+\d+(?:\.\d+)?', sentence)
        found_terms.extend(numbers)
        
        return found_terms
    
    def _create_semantic_chunks(self, sentences: List[Dict], sections: List[Dict]) -> List[Dict]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"""
        chunks = []
        current_chunk = {
            'sentences': [],
            'total_length': 0,
            'type': 'text',
            'section': '',
            'importance': 'medium',
            'key_terms': []
        }
        
        for sentence in sentences:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—Ä–µ–≤—ã—Å–∏–º –ª–∏ —Ä–∞–∑–º–µ—Ä
            potential_length = current_chunk['total_length'] + sentence['length']
            
            if potential_length > self.target_chunk_size and current_chunk['sentences']:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                chunks.append(self._finalize_chunk(current_chunk))
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                current_chunk = self._start_new_chunk_with_overlap(current_chunk, sentence)
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                current_chunk['sentences'].append(sentence)
                current_chunk['total_length'] += sentence['length']
                current_chunk['key_terms'].extend(sentence['key_terms'])
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å —á–∞–Ω–∫–∞
                if sentence['importance'] == 'high':
                    current_chunk['importance'] = 'high'
                elif sentence['importance'] == 'medium' and current_chunk['importance'] == 'low':
                    current_chunk['importance'] = 'medium'
        
        # –ù–µ –∑–∞–±—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk['sentences']:
            chunks.append(self._finalize_chunk(current_chunk))
        
        return chunks
    
    def _finalize_chunk(self, chunk: Dict) -> Dict:
        """–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–∞"""
        text = ' '.join([s['text'] for s in chunk['sentences']])
        
        return {
            'text': text,
            'type': chunk['type'],
            'section': chunk['section'],
            'importance': chunk['importance'],
            'key_terms': list(set(chunk['key_terms'])),  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            'sentence_count': len(chunk['sentences'])
        }
    
    def _start_new_chunk_with_overlap(self, prev_chunk: Dict, new_sentence: Dict) -> Dict:
        """–ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å —É–º–Ω—ã–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º"""
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        overlap_sentences = []
        overlap_length = 0
        
        for sentence in reversed(prev_chunk['sentences']):
            if overlap_length + sentence['length'] <= self.overlap:
                overlap_sentences.insert(0, sentence)
                overlap_length += sentence['length']
            else:
                break
        
        return {
            'sentences': overlap_sentences + [new_sentence],
            'total_length': overlap_length + new_sentence['length'],
            'type': 'text',
            'section': '',
            'importance': new_sentence['importance'],
            'key_terms': new_sentence['key_terms'].copy()
        }
    
    def _optimize_chunk_sizes(self, chunks: List[Dict]) -> List[Dict]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤ —á–∞–Ω–∫–æ–≤"""
        optimized = []
        
        for chunk in chunks:
            # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —á–∞–Ω–∫–∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Å–æ—Å–µ–¥–Ω–∏–º–∏
            if len(chunk['text']) < 100 and optimized:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º
                prev_chunk = optimized[-1]
                prev_chunk['text'] += ' ' + chunk['text']
                prev_chunk['key_terms'].extend(chunk['key_terms'])
                prev_chunk['key_terms'] = list(set(prev_chunk['key_terms']))
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å
                if chunk['importance'] == 'high':
                    prev_chunk['importance'] = 'high'
            else:
                optimized.append(chunk)
        
        return optimized

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–º–Ω–æ–≥–æ —á–∞–Ω–∫–µ—Ä–∞
if __name__ == "__main__":
    # –¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞—É—á–Ω—ã–π —Ç–µ–∫—Å—Ç
    test_text = """
    Lysobacter capsici sp. nov. is a gram-negative, aerobic bacterium isolated from the rhizosphere of pepper plants. 
    The type strain YC5194T was isolated from soil samples collected in South Korea. 
    Cells are rod-shaped, measuring 0.3-0.5 √ó 2.0-20 Œºm, and are motile by means of a polar flagellum.
    
    The temperature range for growth is 15-37¬∞C, with optimal growth at 28¬∞C. 
    The pH range is 5.5-8.5, with optimal growth at pH 7.0. 
    The strain is catalase-positive and oxidase-positive.
    
    The G+C content of the genomic DNA is 65.4 mol%. 
    16S rRNA gene sequence analysis showed highest similarity to Lysobacter gummosus with 97.8% identity.
    
    Phylogenetic analysis based on 16S rRNA sequences placed the strain in the genus Lysobacter.
    The strain shows antimicrobial activity against various plant pathogens.
    """
    
    chunker = ScientificTextChunker(target_chunk_size=300, overlap=50)
    chunks = chunker.split_into_smart_chunks(test_text, "test.pdf")
    
    print("üß¨ –¢–ï–°–¢ –£–ú–ù–û–ì–û –ß–ê–ù–ö–ò–ù–ì–ê")
    print("=" * 50)
    
    for i, chunk in enumerate(chunks, 1):
        print(f"\nüìù –ß–∞–Ω–∫ {i}:")
        print(f"   –î–ª–∏–Ω–∞: {len(chunk['text'])} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –í–∞–∂–Ω–æ—Å—Ç—å: {chunk['metadata']['scientific_importance']}")
        print(f"   –ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: {', '.join(chunk['metadata']['key_terms'][:5])}")
        print(f"   –¢–µ–∫—Å—Ç: {chunk['text'][:100]}...")
    
    print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —É–º–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
    avg_length = sum(len(c['text']) for c in chunks) / len(chunks)
    print(f"üìä –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤") 