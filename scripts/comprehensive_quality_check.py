#!/usr/bin/env python3
"""
–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö —à—Ç–∞–º–º–æ–≤ –ª–∏–∑–æ–±–∞–∫—Ç–µ—Ä–∏–π
"""
import sys
import re
from pathlib import Path
from collections import defaultdict
import json

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
sys.path.insert(0, str(Path(__file__).parent.parent))

def comprehensive_quality_check():
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    print("üîç –ö–û–ú–ü–õ–ï–ö–°–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–•")
    print("=" * 60)
    
    try:
        from lysobacter_rag.indexer.indexer import Indexer
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å–µ—Ä
        indexer = Indexer()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã
        stats = indexer.get_collection_stats()
        total_chunks = stats.get('total_chunks', 0)
        
        print(f"üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"   –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {total_chunks}")
        print(f"   –¢–∏–ø—ã —á–∞–Ω–∫–æ–≤: {stats.get('chunk_types', {})}")
        print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {stats.get('unique_sources', 0)}")
        
        # –°–ø–∏—Å–æ–∫ –æ–±—â–∏—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
        test_queries = [
            "temperature growth", "pH range", "NaCl tolerance", 
            "genome size", "G+C content", "oxidase", "catalase",
            "fatty acids", "quinones", "Antarctic", "soil",
            "marine", "alkaline", "sp. nov", "type strain"
        ]
        
        print(f"\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ö–ê–ß–ï–°–¢–í–ê –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:")
        
        quality_issues = defaultdict(int)
        total_results = 0
        problematic_chunks = []
        
        for query in test_queries:
            print(f"\nüìù –ö–∞—Ç–µ–≥–æ—Ä–∏—è: '{query}'")
            results = indexer.search(query, top_k=10)
            
            if results:
                print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                total_results += len(results)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                for i, result in enumerate(results[:5], 1):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ø-5
                    text = result['text']
                    issues = analyze_text_quality(text)
                    
                    if issues:
                        problematic_chunks.append({
                            'query': query,
                            'rank': i,
                            'text_preview': text[:100] + "...",
                            'issues': issues,
                            'relevance': result.get('relevance_score', 0)
                        })
                        
                        for issue_type, count in issues.items():
                            quality_issues[issue_type] += count
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑
                    if issues:
                        issue_summary = ", ".join([f"{k}:{v}" for k, v in issues.items()])
                        print(f"      {i}. ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã: {issue_summary}")
                    else:
                        print(f"      {i}. ‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ OK")
            else:
                print(f"   ‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        # –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
        print(f"\nüìä –û–ë–©–ò–ô –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê:")
        print(f"   –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {total_results}")
        print(f"   –ü—Ä–æ–±–ª–µ–º–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {len(problematic_chunks)}")
        
        if quality_issues:
            print(f"\n‚ö†Ô∏è –ù–ê–ô–î–ï–ù–ù–´–ï –ü–†–û–ë–õ–ï–ú–´:")
            for issue_type, count in sorted(quality_issues.items(), key=lambda x: x[1], reverse=True):
                print(f"   ‚Ä¢ {issue_type}: {count} —Å–ª—É—á–∞–µ–≤")
        
        # –ê–Ω–∞–ª–∏–∑ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –Ω–∞—É—á–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        print(f"\nüß¨ –ü–†–û–í–ï–†–ö–ê –ù–ê–£–ß–ù–´–• –¢–ï–†–ú–ò–ù–û–í:")
        scientific_terms = [
            "Lysobacter", "sp. nov", "type strain", "16S rRNA",
            "DNA-DNA hybridization", "phylogenetic", "chemotaxonomic",
            "phenotypic", "genotypic", "taxonomy"
        ]
        
        scientific_quality = {}
        for term in scientific_terms:
            results = indexer.search(term, top_k=5)
            if results:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
                correct_usage = 0
                for result in results:
                    if check_scientific_term_usage(result['text'], term):
                        correct_usage += 1
                
                accuracy = (correct_usage / len(results)) * 100
                scientific_quality[term] = accuracy
                status = "‚úÖ" if accuracy > 80 else "‚ö†Ô∏è" if accuracy > 50 else "‚ùå"
                print(f"   {status} {term}: {accuracy:.0f}% –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —à—Ç–∞–º–º–æ–≤—ã—Ö –Ω–æ–º–µ—Ä–æ–≤
        print(f"\nüß™ –ü–†–û–í–ï–†–ö–ê –®–¢–ê–ú–ú–û–í–´–• –ù–û–ú–ï–†–û–í:")
        strain_patterns = [
            r'\w+\d+-\d+T',  # –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω —Ç–∏–ø–∞ GW1-59T
            r'\w+\s+\d+T',   # –ü–∞—Ç—Ç–µ—Ä–Ω —Ç–∏–ø–∞ KCTC 12131T
            r'DSM\s+\d+',    # Deutsche Sammlung von Mikroorganismen
            r'ATCC\s+\d+'    # American Type Culture Collection
        ]
        
        strain_issues = check_strain_nomenclature(indexer)
        
        for pattern_name, issues in strain_issues.items():
            if issues > 0:
                print(f"   ‚ö†Ô∏è {pattern_name}: {issues} –ø—Ä–æ–±–ª–µ–º")
            else:
                print(f"   ‚úÖ {pattern_name}: –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        print(f"\nüî¢ –ü–†–û–í–ï–†–ö–ê –ß–ò–°–õ–û–í–´–• –î–ê–ù–ù–´–•:")
        numerical_quality = check_numerical_data_quality(indexer)
        
        for data_type, quality_score in numerical_quality.items():
            status = "‚úÖ" if quality_score > 0.8 else "‚ö†Ô∏è" if quality_score > 0.5 else "‚ùå"
            print(f"   {status} {data_type}: {quality_score:.1%}")
        
        # –ò—Ç–æ–≥–æ–≤—ã–π —Å–∫–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞
        overall_quality = calculate_overall_quality_score(
            quality_issues, total_results, scientific_quality, 
            strain_issues, numerical_quality
        )
        
        print(f"\nüìà –ò–¢–û–ì–û–í–ê–Ø –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê:")
        print(f"   üéØ –û–±—â–∏–π —Å–∫–æ—Ä: {overall_quality:.0f}/100")
        
        if overall_quality >= 80:
            print("   üéâ –û–¢–õ–ò–ß–ù–û–ï –ö–ê–ß–ï–°–¢–í–û!")
            status = "excellent"
        elif overall_quality >= 60:
            print("   ‚úÖ –•–û–†–û–®–ï–ï –ö–ê–ß–ï–°–¢–í–û")
            status = "good"
        elif overall_quality >= 40:
            print("   ‚ö†Ô∏è –°–†–ï–î–ù–ï–ï –ö–ê–ß–ï–°–¢–í–û - –Ω—É–∂–Ω—ã —É–ª—É—á—à–µ–Ω–∏—è")
            status = "medium"
        else:
            print("   üö® –ù–ò–ó–ö–û–ï –ö–ê–ß–ï–°–¢–í–û - —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è!")
            status = "poor"
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ:")
        recommendations = generate_improvement_recommendations(
            quality_issues, scientific_quality, strain_issues, numerical_quality
        )
        
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        save_quality_report({
            'overall_quality': overall_quality,
            'status': status,
            'total_chunks': total_chunks,
            'quality_issues': dict(quality_issues),
            'scientific_quality': scientific_quality,
            'strain_issues': dict(strain_issues),
            'numerical_quality': numerical_quality,
            'recommendations': recommendations,
            'problematic_chunks': problematic_chunks[:10]  # –¢–æ–ø-10 –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö
        })
        
        return status != "poor"
        
    except Exception as e:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def analyze_text_quality(text):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    issues = {}
    
    # –†–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ —à—Ç–∞–º–º–æ–≤—ã–µ –Ω–æ–º–µ—Ä–∞
    broken_strains = len(re.findall(r'\w+\s*-\s*\d+\s+\w*T', text))
    if broken_strains > 0:
        issues['—Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ_—à—Ç–∞–º–º—ã'] = broken_strains
    
    # –†–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ —Ö–∏–º–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª—ã
    broken_formulas = len(re.findall(r'C\s+\d+\s*:\s*\d+', text))
    if broken_formulas > 0:
        issues['—Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ_—Ñ–æ—Ä–º—É–ª—ã'] = broken_formulas
    
    # –°–ª–∏—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ (–¥–ª–∏–Ω–Ω–µ–µ 50 —Å–∏–º–≤–æ–ª–æ–≤)
    long_words = [w for w in text.split() if len(w) > 50]
    if long_words:
        issues['—Å–ª–∏—Ç–Ω—ã–µ_—Å–ª–æ–≤–∞'] = len(long_words)
    
    # –ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ —á–∏—Å–ª–∞
    broken_numbers = len(re.findall(r'\d+\s+\.\s+\d+', text))
    if broken_numbers > 0:
        issues['–ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ_—á–∏—Å–ª–∞'] = broken_numbers
    
    # –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
    broken_units = len(re.findall(r'\d+\s+¬∞\s+C|\d+\s+%\s+\w+', text))
    if broken_units > 0:
        issues['—Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ_–µ–¥–∏–Ω–∏—Ü—ã'] = broken_units
    
    return issues

def check_scientific_term_usage(text, term):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–∞—É—á–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞"""
    
    # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    term_lower = term.lower()
    text_lower = text.lower()
    
    if term_lower not in text_lower:
        return False
    
    # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
    if term == "sp. nov":
        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏—è –≤–∏–¥–∞
        return bool(re.search(r'\w+\s+\w+\s+sp\.\s+nov', text, re.IGNORECASE))
    
    elif term == "type strain":
        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å —à—Ç–∞–º–º–æ–≤—ã–º –Ω–æ–º–µ—Ä–æ–º
        return bool(re.search(r'type\s+strain.*[A-Z]+\d+', text, re.IGNORECASE))
    
    elif term == "16S rRNA":
        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        return any(keyword in text_lower for keyword in [
            'sequence', 'gene', 'phylogen', 'analysis', 'similarity'
        ])
    
    return True  # –ë–∞–∑–æ–≤–æ–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ —Ç–µ—Ä–º–∏–Ω–∞

def check_strain_nomenclature(indexer):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã —à—Ç–∞–º–º–æ–≤"""
    
    issues = defaultdict(int)
    
    # –ü–æ–∏—Å–∫ —à—Ç–∞–º–º–æ–≤ —Å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º–∏ –ø—Ä–æ–±–ª–µ–º–∞–º–∏
    strain_results = indexer.search("strain type T", top_k=20)
    
    for result in strain_results:
        text = result['text']
        
        # –ò—â–µ–º —Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ —à—Ç–∞–º–º–æ–≤—ã–µ –Ω–æ–º–µ—Ä–∞
        broken_patterns = [
            r'\w+\s*-\s*\d+\s+T',  # GW1- 59T
            r'\w+\d+\s*-\s*\d+\s+T',  # KCTC 12131- T
        ]
        
        for pattern in broken_patterns:
            matches = re.findall(pattern, text)
            if matches:
                issues['—Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ_–Ω–æ–º–µ—Ä–∞'] += len(matches)
        
        # –ò—â–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        if re.search(r'[a-z]+\d+[a-z]+', text):  # –°–ª–∏—Ç–Ω—ã–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è
            issues['—Å–ª–∏—Ç–Ω—ã–µ_–Ω–æ–º–µ—Ä–∞'] += 1
    
    return issues

def check_numerical_data_quality(indexer):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    quality_scores = {}
    
    # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    temp_results = indexer.search("temperature ¬∞C growth", top_k=10)
    temp_quality = 0
    if temp_results:
        correct_temp = sum(1 for r in temp_results 
                          if re.search(r'\d+[-‚Äì]\d+¬∞C', r['text']))
        temp_quality = correct_temp / len(temp_results)
    quality_scores['—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞'] = temp_quality
    
    # pH –¥–∞–Ω–Ω—ã–µ
    ph_results = indexer.search("pH range growth", top_k=10)
    ph_quality = 0
    if ph_results:
        correct_ph = sum(1 for r in ph_results 
                        if re.search(r'pH\s+\d+\.?\d*[-‚Äì]\d+\.?\d*', r['text']))
        ph_quality = correct_ph / len(ph_results)
    quality_scores['pH'] = ph_quality
    
    # –†–∞–∑–º–µ—Ä –≥–µ–Ω–æ–º–∞
    genome_results = indexer.search("genome size Mb", top_k=10)
    genome_quality = 0
    if genome_results:
        correct_genome = sum(1 for r in genome_results 
                           if re.search(r'\d+\.?\d*\s*Mb', r['text']))
        genome_quality = correct_genome / len(genome_results)
    quality_scores['–≥–µ–Ω–æ–º'] = genome_quality
    
    return quality_scores

def calculate_overall_quality_score(quality_issues, total_results, 
                                  scientific_quality, strain_issues, numerical_quality):
    """–í—ã—á–∏—Å–ª—è–µ—Ç –æ–±—â–∏–π —Å–∫–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞"""
    
    # –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä (–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–±–ª–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏—è)
    if total_results > 0:
        extraction_score = max(0, 100 - (sum(quality_issues.values()) / total_results * 100))
    else:
        extraction_score = 0
    
    # –ù–∞—É—á–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å
    science_score = sum(scientific_quality.values()) / len(scientific_quality) if scientific_quality else 0
    
    # –ö–∞—á–µ—Å—Ç–≤–æ —à—Ç–∞–º–º–æ–≤—ã—Ö –Ω–æ–º–µ—Ä–æ–≤
    strain_score = max(0, 100 - sum(strain_issues.values()) * 5)
    
    # –ö–∞—á–µ—Å—Ç–≤–æ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    numerical_score = sum(numerical_quality.values()) / len(numerical_quality) * 100 if numerical_quality else 0
    
    # –í–∑–≤–µ—à–µ–Ω–Ω—ã–π –∏—Ç–æ–≥–æ–≤—ã–π —Å–∫–æ—Ä
    overall_score = (
        extraction_score * 0.4 +  # 40% - –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        science_score * 0.3 +     # 30% - –Ω–∞—É—á–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å
        strain_score * 0.2 +      # 20% - —à—Ç–∞–º–º–æ–≤—ã–µ –Ω–æ–º–µ—Ä–∞
        numerical_score * 0.1     # 10% - —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    )
    
    return max(0, min(100, overall_score))

def generate_improvement_recommendations(quality_issues, scientific_quality, 
                                       strain_issues, numerical_quality):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
    
    recommendations = []
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–±–ª–µ–º–∞–º –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
    if quality_issues:
        top_issue = max(quality_issues.items(), key=lambda x: x[1])
        if top_issue[1] > 5:
            recommendations.append(f"üîß –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –∏—Å–ø—Ä–∞–≤–∏—Ç—å {top_issue[0]} ({top_issue[1]} —Å–ª—É—á–∞–µ–≤)")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–∞—É—á–Ω—ã–º —Ç–µ—Ä–º–∏–Ω–∞–º
    low_science_terms = [term for term, score in scientific_quality.items() if score < 60]
    if low_science_terms:
        recommendations.append(f"üìö –£–ª—É—á—à–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ—Ä–º–∏–Ω–æ–≤: {', '.join(low_science_terms[:3])}")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —à—Ç–∞–º–º–∞–º
    if sum(strain_issues.values()) > 3:
        recommendations.append("üß™ –î–æ–±–∞–≤–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —à—Ç–∞–º–º–æ–≤—ã—Ö –Ω–æ–º–µ—Ä–æ–≤")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
    low_numerical = [data for data, score in numerical_quality.items() if score < 0.6]
    if low_numerical:
        recommendations.append(f"üî¢ –£–ª—É—á—à–∏—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {', '.join(low_numerical)}")
    
    # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if not recommendations:
        recommendations.append("‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ —Ö–æ—Ä–æ—à–µ–µ, –º–µ–ª–∫–∏–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏")
    else:
        recommendations.append("üöÄ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–º")
    
    return recommendations

def save_quality_report(report):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ"""
    
    report_path = Path("quality_report.json")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nüìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")

if __name__ == "__main__":
    success = comprehensive_quality_check()
    
    if not success:
        print("\nüí° –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
        print("1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ 'make fix-extraction' –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è")
        print("2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –æ—à–∏–±–æ–∫")
        print("3. –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞")
    
    sys.exit(0 if success else 1) 