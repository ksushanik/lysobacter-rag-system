#!/usr/bin/env python3
"""
–°–∏—Å—Ç–µ–º–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫–∞—á–µ—Å—Ç–≤–∞ RAG-—Å–∏—Å—Ç–µ–º—ã
"""
import sys
import time
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
sys.path.insert(0, str(Path(__file__).parent.parent))

class QualityMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ RAG-—Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self):
        self.metrics_file = Path("quality_metrics.json")
        self.alerts_file = Path("quality_alerts.json")
        self.history = self._load_history()
        
    def _load_history(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –º–µ—Ç—Ä–∏–∫"""
        if self.metrics_file.exists():
            with open(self.metrics_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []
    
    def _save_history(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –º–µ—Ç—Ä–∏–∫"""
        with open(self.metrics_file, 'w', encoding='utf-8') as f:
            json.dump(self.history, f, ensure_ascii=False, indent=2)
    
    def run_quality_check(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É –∫–∞—á–µ—Å—Ç–≤–∞"""
        
        print("üìä –ú–û–ù–ò–¢–û–†–ò–ù–ì –ö–ê–ß–ï–°–¢–í–ê RAG-–°–ò–°–¢–ï–ú–´")
        print("=" * 50)
        
        try:
            from lysobacter_rag.indexer.indexer import Indexer
            from lysobacter_rag.quality_control.text_enhancer import ScientificTextEnhancer
            
            indexer = Indexer()
            enhancer = ScientificTextEnhancer()
            
            # –¢–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è
            timestamp = datetime.now().isoformat()
            
            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã
            system_metrics = self._collect_system_metrics(indexer)
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
            quality_metrics = self._collect_quality_metrics(indexer, enhancer)
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            performance_metrics = self._collect_performance_metrics(indexer)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏
            current_metrics = {
                'timestamp': timestamp,
                'system': system_metrics,
                'quality': quality_metrics,
                'performance': performance_metrics
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
            self.history.append(current_metrics)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 –∑–∞–ø–∏—Å–µ–π)
            if len(self.history) > 100:
                self.history = self.history[-100:]
            
            self._save_history()
            
            # –ê–Ω–∞–ª–∏–∑ –∏ –æ–ø–æ–≤–µ—â–µ–Ω–∏—è
            alerts = self._analyze_metrics(current_metrics)
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self._display_results(current_metrics, alerts)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            recommendations = self._generate_recommendations(current_metrics, alerts)
            
            if recommendations:
                print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
                for i, rec in enumerate(recommendations, 1):
                    print(f"   {i}. {rec}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _collect_system_metrics(self, indexer) -> Dict:
        """–°–æ–±–∏—Ä–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏"""
        
        try:
            stats = indexer.get_collection_stats()
            return {
                'total_chunks': stats.get('total_chunks', 0),
                'unique_sources': stats.get('unique_sources', 0),
                'chunk_types': stats.get('chunk_types', {}),
                'index_health': 'healthy' if stats.get('total_chunks', 0) > 0 else 'empty'
            }
        except Exception as e:
            return {
                'error': str(e),
                'index_health': 'error'
            }
    
    def _collect_quality_metrics(self, indexer, enhancer) -> Dict:
        """–°–æ–±–∏—Ä–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_tests = [
            {'query': 'GW1-59T', 'category': 'strain_accuracy'},
            {'query': 'temperature growth', 'category': 'data_format'},
            {'query': 'pH range', 'category': 'units'},
            {'query': 'fatty acids', 'category': 'chemistry'},
            {'query': 'type strain', 'category': 'terminology'}
        ]
        
        metrics = {
            'quality_scores': {},
            'common_issues': {},
            'test_results': {}
        }
        
        total_score = 0
        total_tests = 0
        
        for test in quality_tests:
            try:
                results = indexer.search(test['query'], top_k=3)
                
                if results:
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    category_score = 0
                    category_issues = []
                    
                    for result in results:
                        text = result['text']
                        score = enhancer.get_quality_score(text)
                        category_score += score
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
                        issues = self._detect_issues(text)
                        category_issues.extend(issues)
                    
                    category_score /= len(results)
                    metrics['quality_scores'][test['category']] = category_score
                    metrics['common_issues'][test['category']] = category_issues
                    
                    total_score += category_score
                    total_tests += 1
                
                metrics['test_results'][test['query']] = {
                    'found_results': len(results),
                    'avg_relevance': sum(r.get('relevance_score', 0) for r in results) / len(results) if results else 0
                }
                
            except Exception as e:
                metrics['test_results'][test['query']] = {'error': str(e)}
        
        # –û–±—â–∏–π —Å–∫–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞
        metrics['overall_quality_score'] = total_score / total_tests if total_tests > 0 else 0
        
        return metrics
    
    def _collect_performance_metrics(self, indexer) -> Dict:
        """–°–æ–±–∏—Ä–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        
        metrics = {}
        
        # –¢–µ—Å—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞
        start_time = time.time()
        try:
            results = indexer.search("test query", top_k=5)
            search_time = time.time() - start_time
            metrics['search_latency'] = search_time
            metrics['search_status'] = 'ok'
        except Exception as e:
            metrics['search_latency'] = -1
            metrics['search_status'] = 'error'
            metrics['search_error'] = str(e)
        
        return metrics
    
    def _detect_issues(self, text: str) -> List[str]:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤ —Ç–µ–∫—Å—Ç–µ"""
        
        issues = []
        
        # –†–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ —à—Ç–∞–º–º—ã
        import re
        if re.search(r'\w+\s*-\s*\d+\s+T', text):
            issues.append('broken_strain_names')
        
        # –†–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º—É–ª—ã
        if re.search(r'C\s+\d+\s*:\s*\d+', text):
            issues.append('broken_formulas')
        
        # –°–ª–∏—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
        if re.search(r'[a-zA-Z]{50,}', text):
            issues.append('merged_words')
        
        # –†–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã
        if re.search(r'\d+\s+¬∞\s+C|\d+\s+%\s+\w+', text):
            issues.append('broken_units')
        
        return issues
    
    def _analyze_metrics(self, metrics: Dict) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–æ–≤–µ—â–µ–Ω–∏—è"""
        
        alerts = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        quality_score = metrics['quality']['overall_quality_score']
        
        if quality_score < 0.4:
            alerts.append({
                'level': 'critical',
                'type': 'quality',
                'message': f'–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {quality_score:.1%}',
                'action': '–¢—Ä–µ–±—É–µ—Ç—Å—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏'
            })
        elif quality_score < 0.6:
            alerts.append({
                'level': 'warning',
                'type': 'quality',
                'message': f'–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {quality_score:.1%}',
                'action': '–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞'
            })
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        search_latency = metrics['performance'].get('search_latency', 0)
        
        if search_latency > 2.0:
            alerts.append({
                'level': 'warning',
                'type': 'performance',
                'message': f'–ú–µ–¥–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫: {search_latency:.2f}s',
                'action': '–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ —Å–∏—Å—Ç–µ–º—É'
            })
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        total_chunks = metrics['system'].get('total_chunks', 0)
        
        if total_chunks == 0:
            alerts.append({
                'level': 'critical',
                'type': 'system',
                'message': '–ò–Ω–¥–µ–∫—Å –ø—É—Å—Ç',
                'action': '–ó–∞–ø—É—Å—Ç–∏—Ç–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤'
            })
        elif total_chunks < 1000:
            alerts.append({
                'level': 'info',
                'type': 'system',
                'message': f'–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –≤ –∏–Ω–¥–µ–∫—Å–µ: {total_chunks} —á–∞–Ω–∫–æ–≤',
                'action': '–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤'
            })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø–æ–≤–µ—â–µ–Ω–∏—è
        if alerts:
            with open(self.alerts_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'timestamp': metrics['timestamp'],
                    'alerts': alerts
                }, f, ensure_ascii=False, indent=2)
        
        return alerts
    
    def _display_results(self, metrics: Dict, alerts: List[Dict]):
        """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        
        print(f"\nüìä –¢–ï–ö–£–©–ò–ï –ú–ï–¢–†–ò–ö–ò:")
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        system = metrics['system']
        print(f"   üìÅ –ß–∞–Ω–∫–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ: {system.get('total_chunks', 0)}")
        print(f"   üìÑ –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {system.get('unique_sources', 0)}")
        print(f"   üè• –°–æ—Å—Ç–æ—è–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞: {system.get('index_health', 'unknown')}")
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        quality = metrics['quality']
        quality_score = quality.get('overall_quality_score', 0)
        quality_status = "üü¢" if quality_score > 0.8 else "üü°" if quality_score > 0.6 else "üî¥"
        print(f"   {quality_status} –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {quality_score:.1%}")
        
        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        performance = metrics['performance']
        search_time = performance.get('search_latency', -1)
        if search_time >= 0:
            perf_status = "üü¢" if search_time < 1.0 else "üü°" if search_time < 2.0 else "üî¥"
            print(f"   {perf_status} –°–∫–æ—Ä–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞: {search_time:.2f}s")
        else:
            print(f"   üî¥ –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞")
        
        # –û–ø–æ–≤–µ—â–µ–Ω–∏—è
        if alerts:
            print(f"\nüö® –û–ü–û–í–ï–©–ï–ù–ò–Ø:")
            for alert in alerts:
                level_icon = {"critical": "üî¥", "warning": "üü°", "info": "üîµ"}
                icon = level_icon.get(alert['level'], "‚ÑπÔ∏è")
                print(f"   {icon} {alert['message']}")
                print(f"      üí° {alert['action']}")
    
    def _generate_recommendations(self, metrics: Dict, alerts: List[Dict]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        
        recommendations = []
        
        # –ù–∞ –æ—Å–Ω–æ–≤–µ –æ–ø–æ–≤–µ—â–µ–Ω–∏–π
        critical_alerts = [a for a in alerts if a['level'] == 'critical']
        warning_alerts = [a for a in alerts if a['level'] == 'warning']
        
        if critical_alerts:
            recommendations.append("üö® –£—Å—Ç—Ä–∞–Ω–∏—Ç–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã: make full-quality-reindex")
        
        if warning_alerts:
            recommendations.append("‚ö†Ô∏è –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–ª—É—á—à–µ–Ω–∏—è: make quick-quality-improvement")
        
        # –ù–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ–Ω–¥–æ–≤
        if len(self.history) >= 2:
            prev_metrics = self.history[-2]
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            prev_quality = prev_metrics.get('quality', {}).get('overall_quality_score', 0)
            curr_quality = metrics['quality']['overall_quality_score']
            
            if curr_quality < prev_quality - 0.1:
                recommendations.append("üìâ –ö–∞—á–µ—Å—Ç–≤–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–µ–¥–∞–≤–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è")
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            prev_latency = prev_metrics.get('performance', {}).get('search_latency', 0)
            curr_latency = metrics['performance'].get('search_latency', 0)
            
            if curr_latency > prev_latency * 1.5:
                recommendations.append("üêå –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–Ω–∏–∂–∞–µ—Ç—Å—è - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–≥—Ä—É–∑–∫—É")
        
        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        quality_score = metrics['quality']['overall_quality_score']
        
        if quality_score < 0.8:
            recommendations.append("üîß –†–µ–≥—É–ª—è—Ä–Ω–æ –∑–∞–ø—É—Å–∫–∞–π—Ç–µ: make check-overall-quality")
        
        if not recommendations:
            recommendations.append("‚úÖ –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ")
        
        return recommendations
    
    def get_quality_trend(self, days: int = 7) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç—Ä–µ–Ω–¥ –∫–∞—á–µ—Å—Ç–≤–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–Ω–∏"""
        
        cutoff_date = datetime.now() - timedelta(days=days)
        
        recent_metrics = [
            m for m in self.history 
            if datetime.fromisoformat(m['timestamp']) > cutoff_date
        ]
        
        if not recent_metrics:
            return {'trend': 'no_data'}
        
        quality_scores = [
            m['quality']['overall_quality_score'] 
            for m in recent_metrics 
            if 'quality' in m and 'overall_quality_score' in m['quality']
        ]
        
        if len(quality_scores) < 2:
            return {'trend': 'insufficient_data'}
        
        # –ü—Ä–æ—Å—Ç–æ–π —Ç—Ä–µ–Ω–¥
        first_half = quality_scores[:len(quality_scores)//2]
        second_half = quality_scores[len(quality_scores)//2:]
        
        avg_first = sum(first_half) / len(first_half)
        avg_second = sum(second_half) / len(second_half)
        
        if avg_second > avg_first + 0.05:
            trend = 'improving'
        elif avg_second < avg_first - 0.05:
            trend = 'declining'
        else:
            trend = 'stable'
        
        return {
            'trend': trend,
            'current_score': quality_scores[-1],
            'average_score': sum(quality_scores) / len(quality_scores),
            'data_points': len(quality_scores)
        }

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    
    monitor = QualityMonitor()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
    success = monitor.run_quality_check()
    
    if success:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç—Ä–µ–Ω–¥
        trend = monitor.get_quality_trend()
        
        if trend['trend'] != 'no_data':
            print(f"\nüìà –¢–†–ï–ù–î –ö–ê–ß–ï–°–¢–í–ê (7 –¥–Ω–µ–π):")
            trend_icons = {
                'improving': 'üìà –£–ª—É—á—à–∞–µ—Ç—Å—è',
                'stable': '‚û°Ô∏è –°—Ç–∞–±–∏–ª—å–Ω–æ',
                'declining': 'üìâ –°–Ω–∏–∂–∞–µ—Ç—Å—è'
            }
            print(f"   {trend_icons.get(trend['trend'], '‚ùì –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            
            if 'current_score' in trend:
                print(f"   –¢–µ–∫—É—â–∏–π —Å–∫–æ—Ä: {trend['current_score']:.1%}")
                print(f"   –°—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä: {trend['average_score']:.1%}")
        
        print(f"\nüìÑ –ü–æ–¥—Ä–æ–±–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ quality_metrics.json")
    
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 