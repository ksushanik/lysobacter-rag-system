"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –Ω–æ–≤–æ–≥–æ PDF —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
"""

from pathlib import Path
from typing import List, Dict, Any
from loguru import logger
from tqdm import tqdm

from .document_processor import DocumentChunk
from ..pdf_extractor.advanced_pdf_extractor import AdvancedPDFExtractor, ExtractedElement
from ..pdf_extractor.text_quality_improver import text_quality_improver


class AdvancedDocumentProcessor:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º PDF"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞"""
        self.pdf_extractor = AdvancedPDFExtractor()
        self.chunk_size = 1000  # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        self.chunk_overlap = 200  # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —á–∞–Ω–∫–æ–≤
        
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    def process_pdf_directory(self, pdf_directory: Path) -> List[DocumentChunk]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å PDF —Ñ–∞–π–ª–∞–º–∏
        
        Args:
            pdf_directory (Path): –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å PDF
            
        Returns:
            List[DocumentChunk]: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        """
        pdf_files = list(pdf_directory.glob("*.pdf"))
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(pdf_files)} PDF —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        all_chunks = []
        
        for pdf_file in tqdm(pdf_files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ PDF —Ñ–∞–π–ª–æ–≤"):
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
                document = self.pdf_extractor.extract_document(pdf_file)
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á–∞–Ω–∫–∏
                chunks = self._convert_document_to_chunks(document)
                all_chunks.extend(chunks)
                
                logger.info(f"‚úÖ {pdf_file.name}: {len(chunks)} —á–∞–Ω–∫–æ–≤")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {pdf_file.name}: {e}")
                continue
        
        logger.info(f"üéâ –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(all_chunks)} —á–∞–Ω–∫–æ–≤")
        return all_chunks
    
    def _convert_document_to_chunks(self, document) -> List[DocumentChunk]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ —á–∞–Ω–∫–∏"""
        
        chunks = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        text_elements = [e for e in document.elements if e.element_type == 'text']
        text_chunks = self._create_text_chunks(text_elements, document.file_path)
        chunks.extend(text_chunks)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—á–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        table_elements = [e for e in document.elements if e.element_type == 'table']
        table_chunks = self._create_table_chunks(table_elements, document.file_path)
        chunks.extend(table_chunks)
        
        return chunks
    
    def _create_text_chunks(self, text_elements: List[ExtractedElement], source_file: str) -> List[DocumentChunk]:
        """–°–æ–∑–¥–∞—ë—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —á–∞–Ω–∫–∏"""
        
        chunks = []
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
        full_text = "\n\n".join([element.content for element in text_elements])
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
        text_chunks = self._split_text_with_overlap(full_text)
        
        for i, chunk_text in enumerate(text_chunks):
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            improved_text = text_quality_improver.improve_text_quality(chunk_text)
            
            chunk = DocumentChunk(
                chunk_id=f"{Path(source_file).stem}_text_{i}",
                text=improved_text,
                chunk_type="text",
                metadata={
                    "source_pdf": Path(source_file).name,
                    "chunk_index": i,
                    "total_chunks": len(text_chunks),
                    "extraction_method": "advanced_pymupdf4llm",
                    "quality_improved": True
                }
            )
            chunks.append(chunk)
        
        return chunks
    
    def _create_table_chunks(self, table_elements: List[ExtractedElement], source_file: str) -> List[DocumentChunk]:
        """–°–æ–∑–¥–∞—ë—Ç —Ç–∞–±–ª–∏—á–Ω—ã–µ —á–∞–Ω–∫–∏"""
        
        chunks = []
        
        for i, table_element in enumerate(table_elements):
            # –£–ª—É—á—à–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ç–∞–±–ª–∏—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            improved_table_text = text_quality_improver.improve_text_quality(table_element.content)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
            enhanced_text = self._enhance_table_context(improved_table_text, table_element)
            
            chunk = DocumentChunk(
                chunk_id=f"{Path(source_file).stem}_table_{i}",
                text=enhanced_text,
                chunk_type="table",
                metadata={
                    "source_pdf": Path(source_file).name,
                    "page_number": table_element.page_number,
                    "table_index": i,
                    "extraction_method": table_element.metadata.get('extraction_method', 'unknown'),
                    "table_shape": table_element.metadata.get('table_shape', ''),
                    "relevance_score": table_element.confidence,
                    "contains_strain_data": table_element.metadata.get('contains_strain_data', False),
                    "quality_improved": True
                }
            )
            chunks.append(chunk)
        
        return chunks
    
    def _enhance_table_context(self, table_text: str, table_element: ExtractedElement) -> str:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫ —Ç–∞–±–ª–∏—á–Ω—ã–º –¥–∞–Ω–Ω—ã–º –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞"""
        
        enhanced_text = table_text
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
        if 'YC5194' in table_text:
            enhanced_text = f"–®–¢–ê–ú–ú YC5194 –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò:\n{enhanced_text}"
        
        if any(word in table_text.lower() for word in ['temperature', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞']):
            enhanced_text = f"–¢–ï–ú–ü–ï–†–ê–¢–£–†–ù–´–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò:\n{enhanced_text}"
        
        if any(word in table_text.lower() for word in ['ph', '–∫–∏—Å–ª–æ—Ç–Ω–æ—Å—Ç—å']):
            enhanced_text = f"pH –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò:\n{enhanced_text}"
        
        if any(word in table_text.lower() for word in ['growth', '—Ä–æ—Å—Ç']):
            enhanced_text = f"–£–°–õ–û–í–ò–Ø –†–û–°–¢–ê:\n{enhanced_text}"
        
        if any(word in table_text.lower() for word in ['morphology', '–º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è']):
            enhanced_text = f"–ú–û–†–§–û–õ–û–ì–ò–ß–ï–°–ö–ò–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò:\n{enhanced_text}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        meta_info = f"\n–ò—Å—Ç–æ—á–Ω–∏–∫: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {table_element.page_number}, "
        meta_info += f"–º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {table_element.metadata.get('extraction_method', 'unknown')}, "
        meta_info += f"—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {table_element.confidence:.2f}"
        
        enhanced_text += meta_info
        
        return enhanced_text
    
    def _split_text_with_overlap(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º"""
        
        if len(text) <= self.chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            if end < len(text):
                for delimiter in ['. ', '\n\n', '\n', '; ']:
                    last_delimiter = text.rfind(delimiter, start, end)
                    if last_delimiter != -1:
                        end = last_delimiter + len(delimiter)
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # –°–ª–µ–¥—É—é—â–∏–π —á–∞–Ω–∫ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
            start = end - self.chunk_overlap
            
            if start >= len(text):
                break
        
        return chunks
    
    def get_processing_stats(self, chunks: List[DocumentChunk]) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        
        text_chunks = [c for c in chunks if c.chunk_type == 'text']
        table_chunks = [c for c in chunks if c.chunk_type == 'table']
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤–∞–∂–Ω—ã—Ö —à—Ç–∞–º–º–æ–≤
        yc5194_mentions = sum(1 for c in chunks if 'YC5194' in c.text)
        strain_mentions = sum(1 for c in chunks if 'strain' in c.text.lower())
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        total_chars = sum(len(c.text) for c in chunks)
        avg_chunk_size = total_chars / len(chunks) if chunks else 0
        
        return {
            'total_chunks': len(chunks),
            'text_chunks': len(text_chunks),
            'table_chunks': len(table_chunks),
            'yc5194_mentions': yc5194_mentions,
            'strain_mentions': strain_mentions,
            'total_characters': total_chars,
            'avg_chunk_size': avg_chunk_size,
            'quality_improved': all(c.metadata.get('quality_improved', False) for c in chunks)
        } 