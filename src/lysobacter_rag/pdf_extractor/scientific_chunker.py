"""
–£–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –æ Lysobacter
–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º PDF —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–º
"""

import re
from typing import List, Dict, Tuple
from loguru import logger


class ScientificTextChunker:
    """–£–º–Ω—ã–π —á–∞–Ω–∫–µ—Ä –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –æ Lysobacter"""
    
    def __init__(self, target_chunk_size: int = 350, overlap: int = 50):
        self.target_chunk_size = target_chunk_size
        self.overlap = overlap
        
        # –ù–∞—É—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è Lysobacter
        self.section_patterns = [
            r"(Description|Morphology|Physiology|Biochemical characteristics)",
            r"(Growth conditions|Temperature range|pH range|Cultivation)",
            r"(Type strain|Strain|Isolate|Culture)",
            r"(G\+C content|DNA composition|Genomic)",
            r"(16S rRNA|Phylogen|Taxonomy|Systematic)",
            r"(Etymology|Diagnosis|Differentiation)"
        ]
        
        # –ö–ª—é—á–µ–≤—ã–µ –Ω–∞—É—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        self.key_terms = [
            "Lysobacter", "sp. nov.", "type strain", "isolate", "strain",
            "G+C content", "16S rRNA", "phylogenetic", "taxonomy",
            "temperature", "pH", "growth", "morphology", "cell",
            "catalase", "oxidase", "gram-negative", "gram-positive",
            "antibiotic", "antimicrobial", "biocontrol", "enzyme",
            "rhizosphere", "soil", "plant", "pathogen"
        ]
        
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É–º–Ω—ã–π —á–∞–Ω–∫–µ—Ä: —Ä–∞–∑–º–µ—Ä {target_chunk_size}, –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ {overlap}")
        
    def chunk_extracted_elements(self, elements: List[Dict]) -> List[Dict]:
        """
        –£–º–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏
        
        Args:
            elements: –°–ø–∏—Å–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –æ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        """
        logger.info(f"üß¨ –ù–∞—á–∏–Ω–∞—é —É–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        
        all_chunks = []
        
        for element in elements:
            if element.get('element_type') == 'table':
                # –¢–∞–±–ª–∏—Ü—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ
                table_chunks = self._chunk_table_element(element)
                all_chunks.extend(table_chunks)
            else:
                # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Ä–∞–∑–±–∏–≤–∞–µ–º —É–º–Ω–æ
                text_chunks = self._chunk_text_element(element)
                all_chunks.extend(text_chunks)
        
        # –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö —á–∞–Ω–∫–æ–≤
        optimized_chunks = self._optimize_chunks(all_chunks)
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(optimized_chunks)} —É–º–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
        logger.info(f"   üìä –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {self._calculate_avg_size(optimized_chunks)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        return optimized_chunks
    
    def _chunk_text_element(self, element: Dict) -> List[Dict]:
        """–£–º–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        
        text = element.get('content', '')
        if len(text) < 100:  # –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            return [self._create_chunk(text, element, 'text_small')]
        
        # 1. –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        cleaned_text = self._clean_scientific_text(text)
        
        # 2. –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = self._split_into_sentences(cleaned_text)
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤
        chunks = self._create_semantic_chunks(sentences, element)
        
        return chunks
    
    def _chunk_table_element(self, element: Dict) -> List[Dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–∞–±–ª–∏—á–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        
        table_text = element.get('content', '')
        
        # –¢–∞–±–ª–∏—Ü—ã –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–µ
        if len(table_text) <= self.target_chunk_size * 1.5:
            return [self._create_chunk(table_text, element, 'table')]
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–∏–µ —Ç–∞–±–ª–∏—Ü—ã –ø–æ —Å—Ç—Ä–æ–∫–∞–º
        lines = table_text.split('\n')
        chunks = []
        current_chunk_lines = []
        current_length = 0
        
        for line in lines:
            line_length = len(line)
            
            if current_length + line_length > self.target_chunk_size and current_chunk_lines:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                chunk_text = '\n'.join(current_chunk_lines)
                chunks.append(self._create_chunk(chunk_text, element, 'table_part'))
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º (–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏)
                overlap_lines = current_chunk_lines[-2:] if len(current_chunk_lines) > 2 else []
                current_chunk_lines = overlap_lines + [line]
                current_length = sum(len(l) for l in current_chunk_lines)
            else:
                current_chunk_lines.append(line)
                current_length += line_length
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk_lines:
            chunk_text = '\n'.join(current_chunk_lines)
            chunks.append(self._create_chunk(chunk_text, element, 'table_part'))
        
        return chunks
    
    def _clean_scientific_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞—É—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
        text = re.sub(r'(\w+)-\s+(\w+)', r'\1\2', text)
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ —á–∏—Å–ª–∞ –∏ –µ–¥–∏–Ω–∏—Ü—ã
        text = re.sub(r'(\d+)\s*¬∞\s*C', r'\1¬∞C', text)
        text = re.sub(r'(\d+)\s*%', r'\1%', text)
        text = re.sub(r'pH\s+(\d+(?:\.\d+)?)', r'pH \1', text)
        text = re.sub(r'(\d+(?:\.\d+)?)\s*mol\s*%', r'\1 mol%', text)
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞—É—á–Ω—ã–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
        text = re.sub(r'\bsp\.\s*nov\.\s*', 'sp. nov. ', text)
        text = re.sub(r'\bgen\.\s*nov\.\s*', 'gen. nov. ', text)
        text = re.sub(r'\bvar\.\s*', 'var. ', text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Å–ª–∏—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ (—á–∞—Å—Ç–æ –≤ OCR)
        text = re.sub(r'([a-z])([A-Z])', r'\1 \2', text)
        
        return text.strip()
    
    def _split_into_sentences(self, text: str) -> List[Dict]:
        """–†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –∞–Ω–∞–ª–∏–∑–æ–º –≤–∞–∂–Ω–æ—Å—Ç–∏"""
        
        # –†–∞–∑–±–∏–≤–∫–∞ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º (—É—á–∏—Ç—ã–≤–∞–µ–º –Ω–∞—É—á–Ω—ã–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è)
        # –ò–∑–±–µ–≥–∞–µ–º lookbehind –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã
        sentences = []
        
        # –°–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–±–∏–≤–∞–µ–º –ø—Ä–æ—Å—Ç—ã–º —Å–ø–æ—Å–æ–±–æ–º
        raw_sentences = re.split(r'(?<=[.!?])\s+', text)
        
        # –ó–∞—Ç–µ–º –æ–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ —Ç–µ, —á—Ç–æ –±—ã–ª–∏ —Ä–∞–∑–æ—Ä–≤–∞–Ω—ã –∏–∑-–∑–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
        scientific_abbrevs = ['sp.', 'var.', 'gen.', 'cf.', 'Fig.', 'Tab.', 'etc.', 'vs.', 'Dr.', 'Prof.']
        
        current_sentence = ""
        for sentence in raw_sentences:
            current_sentence += sentence
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ª–∏ –Ω–∞ –Ω–∞—É—á–Ω–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ
            is_abbrev = any(current_sentence.strip().endswith(abbrev) for abbrev in scientific_abbrevs)
            
            if not is_abbrev:
                # –≠—Ç–æ –ø–æ–ª–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                sentences.append(current_sentence.strip())
                current_sentence = ""
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–±–µ–ª –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                current_sentence += " "
        
        # –ù–µ –∑–∞–±—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        if current_sentence.strip():
            sentences.append(current_sentence.strip())
        
        result = []
        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if len(sentence) < 10:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ
                continue
                
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            importance = self._analyze_sentence_importance(sentence)
            key_terms = self._extract_key_terms(sentence)
            
            result.append({
                'text': sentence,
                'index': i,
                'importance': importance,
                'key_terms': key_terms,
                'length': len(sentence)
            })
            
        return result
    
    def _analyze_sentence_importance(self, sentence: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è"""
        
        sentence_lower = sentence.lower()
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        critical_patterns = [
            r'type strain.*isolated', r'strain.*isolated from',
            r'g\+c content.*\d+', r'dna.*\d+.*mol%',
            r'temperature.*range.*\d+.*¬∞c', r'growth.*\d+.*¬∞c',
            r'ph.*range.*\d+', r'ph.*\d+.*\d+',
            r'cell.*size.*\d+', r'cells.*\d+.*Œºm'
        ]
        
        # –í—ã—Å–æ–∫–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å
        high_patterns = [
            r'sp\.\s*nov\.', r'type strain', r'isolate',
            r'16s rrna', r'phylogenetic', r'taxonomy',
            r'gram-negative', r'gram-positive',
            r'catalase.*positive', r'oxidase.*positive',
            r'morphology', r'biochemical'
        ]
        
        # –°—Ä–µ–¥–Ω—è—è –≤–∞–∂–Ω–æ—Å—Ç—å  
        medium_patterns = [
            r'growth', r'cultivation', r'medium',
            r'antibiotic', r'antimicrobial', r'activity',
            r'sequence', r'similarity', r'identity'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –ø–æ—Ä—è–¥–∫–µ –≤–∞–∂–Ω–æ—Å—Ç–∏
        for pattern in critical_patterns:
            if re.search(pattern, sentence_lower):
                return 'critical'
        
        for pattern in high_patterns:
            if re.search(pattern, sentence_lower):
                return 'high'
        
        for pattern in medium_patterns:
            if re.search(pattern, sentence_lower):
                return 'medium'
        
        return 'low'
    
    def _extract_key_terms(self, sentence: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        found_terms = []
        
        sentence_lower = sentence.lower()
        for term in self.key_terms:
            if term.lower() in sentence_lower:
                found_terms.append(term)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        scientific_data = []
        
        # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã
        temps = re.findall(r'\d+(?:\.\d+)?¬∞C', sentence)
        scientific_data.extend(temps)
        
        # pH –∑–Ω–∞—á–µ–Ω–∏—è
        ph_vals = re.findall(r'pH\s+\d+(?:\.\d+)?', sentence)
        scientific_data.extend(ph_vals)
        
        # G+C —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
        gc_vals = re.findall(r'\d+(?:\.\d+)?\s*mol%', sentence)
        scientific_data.extend(gc_vals)
        
        # –†–∞–∑–º–µ—Ä—ã –∫–ª–µ—Ç–æ–∫
        sizes = re.findall(r'\d+(?:\.\d+)?[-√ó]\d+(?:\.\d+)?\s*Œºm', sentence)
        scientific_data.extend(sizes)
        
        found_terms.extend(scientific_data)
        
        return found_terms
    
    def _create_semantic_chunks(self, sentences: List[Dict], original_element: Dict) -> List[Dict]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"""
        
        if not sentences:
            return []
        
        chunks = []
        current_chunk = {
            'sentences': [],
            'total_length': 0,
            'importance': 'low',
            'key_terms': []
        }
        
        for sentence in sentences:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
            potential_length = current_chunk['total_length'] + sentence['length']
            
            if potential_length > self.target_chunk_size and current_chunk['sentences']:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                chunks.append(self._finalize_semantic_chunk(current_chunk, original_element))
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —Å —É–º–Ω—ã–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                current_chunk = self._start_new_chunk_with_overlap(current_chunk, sentence)
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                current_chunk['sentences'].append(sentence)
                current_chunk['total_length'] += sentence['length']
                current_chunk['key_terms'].extend(sentence['key_terms'])
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å
                if sentence['importance'] in ['critical', 'high']:
                    current_chunk['importance'] = sentence['importance']
                elif sentence['importance'] == 'medium' and current_chunk['importance'] == 'low':
                    current_chunk['importance'] = 'medium'
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk['sentences']:
            chunks.append(self._finalize_semantic_chunk(current_chunk, original_element))
        
        return chunks
    
    def _finalize_semantic_chunk(self, chunk: Dict, original_element: Dict) -> Dict:
        """–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–∞"""
        
        text = ' '.join([s['text'] for s in chunk['sentences']])
        
        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = original_element.get('metadata', {}).copy()
        metadata.update({
            'chunk_type': 'text',
            'scientific_importance': chunk['importance'],
            'key_terms': list(set(chunk['key_terms'])),  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            'sentence_count': len(chunk['sentences']),
            'chunking_method': 'semantic'
        })
        
        return {
            'content': text,
            'metadata': metadata,
            'page_number': original_element.get('page_number', 1),
            'confidence': original_element.get('confidence', 0.8)
        }
    
    def _start_new_chunk_with_overlap(self, prev_chunk: Dict, new_sentence: Dict) -> Dict:
        """–ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å —É–º–Ω—ã–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º"""
        
        # –ë–µ—Ä–µ–º –≤–∞–∂–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        overlap_sentences = []
        overlap_length = 0
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≤–∞–∂–Ω—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
        important_sentences = [s for s in prev_chunk['sentences'] 
                             if s['importance'] in ['critical', 'high']]
        
        if important_sentences:
            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            for sentence in reversed(important_sentences[-2:]):
                if overlap_length + sentence['length'] <= self.overlap:
                    overlap_sentences.insert(0, sentence)
                    overlap_length += sentence['length']
        else:
            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            for sentence in reversed(prev_chunk['sentences'][-2:]):
                if overlap_length + sentence['length'] <= self.overlap:
                    overlap_sentences.insert(0, sentence)
                    overlap_length += sentence['length']
        
        return {
            'sentences': overlap_sentences + [new_sentence],
            'total_length': overlap_length + new_sentence['length'],
            'importance': new_sentence['importance'],
            'key_terms': new_sentence['key_terms'].copy()
        }
    
    def _create_chunk(self, text: str, original_element: Dict, chunk_type: str) -> Dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ —á–∞–Ω–∫–∞"""
        
        metadata = original_element.get('metadata', {}).copy()
        metadata.update({
            'chunk_type': chunk_type,
            'chunking_method': 'simple'
        })
        
        if chunk_type.startswith('table'):
            metadata['key_terms'] = self._extract_key_terms(text)
            metadata['scientific_importance'] = 'high'  # –¢–∞–±–ª–∏—Ü—ã –æ–±—ã—á–Ω–æ –≤–∞–∂–Ω—ã
        
        return {
            'content': text,
            'metadata': metadata,
            'page_number': original_element.get('page_number', 1),
            'confidence': original_element.get('confidence', 0.8)
        }
    
    def _optimize_chunks(self, chunks: List[Dict]) -> List[Dict]:
        """–ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤ —á–∞–Ω–∫–æ–≤"""
        
        if not chunks:
            return chunks
        
        optimized = []
        
        for chunk in chunks:
            text_length = len(chunk['content'])
            
            # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —á–∞–Ω–∫–∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏
            if text_length < 100 and optimized:
                prev_chunk = optimized[-1]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
                if self._chunks_compatible(prev_chunk, chunk):
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º
                    combined_text = prev_chunk['content'] + ' ' + chunk['content']
                    prev_chunk['content'] = combined_text
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
                    prev_terms = prev_chunk['metadata'].get('key_terms', [])
                    curr_terms = chunk['metadata'].get('key_terms', [])
                    prev_chunk['metadata']['key_terms'] = list(set(prev_terms + curr_terms))
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å
                    prev_importance = prev_chunk['metadata'].get('scientific_importance', 'low')
                    curr_importance = chunk['metadata'].get('scientific_importance', 'low')
                    
                    importance_order = ['low', 'medium', 'high', 'critical']
                    if importance_order.index(curr_importance) > importance_order.index(prev_importance):
                        prev_chunk['metadata']['scientific_importance'] = curr_importance
                    
                    continue
            
            optimized.append(chunk)
        
        return optimized
    
    def _chunks_compatible(self, chunk1: Dict, chunk2: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è"""
        
        # –°–æ–≤–º–µ—Å—Ç–∏–º—ã –µ—Å–ª–∏:
        # 1. –° –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        # 2. –û–¥–∏–Ω–∞–∫–æ–≤—ã–π —Ç–∏–ø (—Ç–µ–∫—Å—Ç —Å —Ç–µ–∫—Å—Ç–æ–º, —Ç–∞–±–ª–∏—Ü–∞ —Å —Ç–∞–±–ª–∏—Ü–µ–π)
        # 3. –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç
        
        same_page = chunk1.get('page_number') == chunk2.get('page_number')
        
        type1 = chunk1['metadata'].get('chunk_type', 'text')
        type2 = chunk2['metadata'].get('chunk_type', 'text')
        compatible_types = (type1.startswith('text') and type2.startswith('text')) or \
                          (type1.startswith('table') and type2.startswith('table'))
        
        combined_length = len(chunk1['content']) + len(chunk2['content'])
        size_ok = combined_length <= self.target_chunk_size * 1.5
        
        return same_page and compatible_types and size_ok
    
    def _calculate_avg_size(self, chunks: List[Dict]) -> int:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–æ–≤"""
        if not chunks:
            return 0
        
        total_size = sum(len(chunk['content']) for chunk in chunks)
        return int(total_size / len(chunks)) 