"""
–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π PDF —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ª—É—á—à–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ —Ç–∞–±–ª–∏—Ü
"""

import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from loguru import logger
import pandas as pd
from tqdm import tqdm

# –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ PDF –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import pymupdf4llm
import pdfplumber
import tabula

from .text_quality_improver import text_quality_improver
from .scientific_chunker import ScientificTextChunker


@dataclass
class ExtractedElement:
    """–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    element_type: str  # 'text', 'table', 'header', 'list'
    content: str
    page_number: int
    confidence: float
    metadata: Dict[str, Any]
    raw_data: Any = None


@dataclass
class AdvancedExtractedDocument:
    """–î–æ–∫—É–º–µ–Ω—Ç, –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞"""
    file_path: str
    title: str
    total_pages: int
    elements: List[ExtractedElement]
    metadata: Dict[str, Any]
    extraction_stats: Dict[str, Any]


class AdvancedPDFExtractor:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π PDF —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ç–∞–±–ª–∏—Ü –∏ –≤—ã—Å–æ–∫–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º"""
    
    def __init__(self, use_smart_chunking: bool = True):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞"""
        self.quality_improver = text_quality_improver
        self.use_smart_chunking = use_smart_chunking
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É–º–Ω—ã–π —á–∞–Ω–∫–µ—Ä
        if use_smart_chunking:
            self.chunker = ScientificTextChunker(target_chunk_size=350, overlap=50)
            logger.info("‚úÖ –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω —É–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ (—Ä–∞–∑–º–µ—Ä: 350, –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: 50)")
        else:
            self.chunker = None
            logger.info("‚ö†Ô∏è –£–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –æ—Ç–∫–ª—é—á—ë–Ω")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self.table_keywords = [
            'table', '—Ç–∞–±–ª–∏—Ü–∞', 'characteristics', '—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏',
            'strain', '—à—Ç–∞–º–º', 'species', '–≤–∏–¥', 'temperature', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞',
            'growth', '—Ä–æ—Å—Ç', 'pH', '–º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è', 'biochemical', '–±–∏–æ—Ö–∏–º–∏—á–µ—Å–∫–∏–π'
        ]
        
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π PDF —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä")
    
    def extract_document(self, pdf_path: Path) -> AdvancedExtractedDocument:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤
        
        Args:
            pdf_path (Path): –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
            
        Returns:
            AdvancedExtractedDocument: –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        """
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞—é –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ: {pdf_path.name}")
        
        elements = []
        extraction_stats = {
            'total_pages': 0,
            'text_elements': 0,
            'table_elements': 0,
            'quality_score': 0,
            'methods_used': []
        }
        
        try:
            # –ú–µ—Ç–æ–¥ 1: pymupdf4llm –¥–ª—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            logger.info("üìù –ò–∑–≤–ª–µ–∫–∞—é —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é pymupdf4llm...")
            text_elements = self._extract_with_pymupdf4llm(pdf_path)
            elements.extend(text_elements)
            extraction_stats['text_elements'] = len(text_elements)
            extraction_stats['methods_used'].append('pymupdf4llm')
            
            # –ú–µ—Ç–æ–¥ 2: pdfplumber –¥–ª—è —Ç–∞–±–ª–∏—Ü
            logger.info("üìä –ò–∑–≤–ª–µ–∫–∞—é —Ç–∞–±–ª–∏—Ü—ã —Å –ø–æ–º–æ—â—å—é pdfplumber...")
            table_elements = self._extract_tables_pdfplumber(pdf_path)
            elements.extend(table_elements)
            extraction_stats['table_elements'] = len(table_elements)
            extraction_stats['methods_used'].append('pdfplumber')
            
            # –ú–µ—Ç–æ–¥ 3: tabula –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü
            logger.info("üìã –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü —Å tabula...")
            tabula_elements = self._extract_tables_tabula(pdf_path)
            elements.extend(tabula_elements)
            extraction_stats['methods_used'].append('tabula')
            
            # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            with pdfplumber.open(pdf_path) as pdf:
                extraction_stats['total_pages'] = len(pdf.pages)
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            total_text_length = sum(len(e.content) for e in elements if e.element_type == 'text')
            quality_score = min(100, max(0, (total_text_length / 1000) * 10))
            extraction_stats['quality_score'] = quality_score
            
            logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            logger.info(f"   üìù –¢–µ–∫—Å—Ç: {extraction_stats['text_elements']}")
            logger.info(f"   üìä –¢–∞–±–ª–∏—Ü—ã: {extraction_stats['table_elements']}")
            logger.info(f"   üìà –ö–∞—á–µ—Å—Ç–≤–æ: {quality_score:.1f}%")
            
            return AdvancedExtractedDocument(
                file_path=str(pdf_path),
                title=pdf_path.stem,
                total_pages=extraction_stats['total_pages'],
                elements=elements,
                metadata={
                    'extraction_timestamp': pd.Timestamp.now().isoformat(),
                    'file_size_mb': pdf_path.stat().st_size / (1024 * 1024),
                    'extractor_version': '3.0_smart_chunking'
                },
                extraction_stats=extraction_stats
            )
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ {pdf_path}: {e}")
            return self._create_fallback_document(pdf_path, str(e))
    
    def get_smart_chunks(self, document: AdvancedExtractedDocument) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —É–º–Ω—ã–µ —á–∞–Ω–∫–∏ –∏–∑ –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Args:
            document: –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ —É–º–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        """
        if not self.use_smart_chunking or not self.chunker:
            logger.warning("‚ö†Ô∏è –£–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –æ—Ç–∫–ª—é—á—ë–Ω, –≤–æ–∑–≤—Ä–∞—â–∞—é —ç–ª–µ–º–µ–Ω—Ç—ã –∫–∞–∫ –µ—Å—Ç—å")
            return self._convert_elements_to_chunks(document.elements)
        
        logger.info(f"üß¨ –ü—Ä–∏–º–µ–Ω—è—é —É–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –∫ {len(document.elements)} —ç–ª–µ–º–µ–Ω—Ç–∞–º")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —á–∞–Ω–∫–µ—Ä–∞
        elements_for_chunking = []
        for element in document.elements:
            elements_for_chunking.append({
                'content': element.content,
                'element_type': element.element_type,
                'page_number': element.page_number,
                'confidence': element.confidence,
                'metadata': element.metadata
            })
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —É–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥
        smart_chunks = self.chunker.chunk_extracted_elements(elements_for_chunking)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        for chunk in smart_chunks:
            chunk['metadata']['source_pdf'] = document.title + '.pdf'
            chunk['metadata']['extractor_version'] = '3.0_smart_chunking'
            chunk['metadata']['total_pages'] = document.total_pages
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(smart_chunks)} —É–º–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
        
        return smart_chunks
    
    def _convert_elements_to_chunks(self, elements: List[ExtractedElement]) -> List[Dict]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –ø—Ä–æ—Å—Ç—ã–µ —á–∞–Ω–∫–∏ (–±–µ–∑ —É–º–Ω–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞)"""
        chunks = []
        
        for element in elements:
            chunk = {
                'content': element.content,
                'metadata': element.metadata.copy(),
                'page_number': element.page_number,
                'confidence': element.confidence
            }
            
            chunk['metadata']['chunk_type'] = element.element_type
            chunk['metadata']['chunking_method'] = 'simple'
            
            chunks.append(chunk)
        
        return chunks
    
    def _extract_with_pymupdf4llm(self, pdf_path: Path) -> List[ExtractedElement]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é pymupdf4llm (–≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ)"""
        
        elements = []
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º pymupdf4llm –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            markdown_text = pymupdf4llm.to_markdown(str(pdf_path))
            
            if not markdown_text or len(markdown_text.strip()) < 50:
                logger.warning("pymupdf4llm –≤–µ—Ä–Ω—É–ª –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥")
                return self._extract_with_basic_pymupdf(pdf_path)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
            pages = self._split_markdown_by_pages(markdown_text)
            
            for page_num, page_content in enumerate(pages, 1):
                if len(page_content.strip()) < 20:
                    continue
                
                # –£–ª—É—á—à–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞
                improved_text = self.quality_improver.improve_text_quality(page_content)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                element_type = self._classify_content(improved_text)
                
                element = ExtractedElement(
                    element_type=element_type,
                    content=improved_text,
                    page_number=page_num,
                    confidence=0.9,  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è pymupdf4llm
                    metadata={
                        'extraction_method': 'pymupdf4llm',
                        'original_length': len(page_content),
                        'improved_length': len(improved_text),
                        'content_type': element_type
                    }
                )
                
                elements.append(element)
            
            logger.info(f"‚úÖ pymupdf4llm –∏–∑–≤–ª—ë–∫ {len(elements)} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ pymupdf4llm: {e}, –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ –±–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥")
            return self._extract_with_basic_pymupdf(pdf_path)
        
        return elements
    
    def _extract_tables_pdfplumber(self, pdf_path: Path) -> List[ExtractedElement]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã —Å –ø–æ–º–æ—â—å—é pdfplumber"""
        
        table_elements = []
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
                    tables = page.extract_tables()
                    
                    for table_idx, table in enumerate(tables):
                        if not table or len(table) < 2:
                            continue
                        
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                        table_text = self._table_to_structured_text(
                            table, page_num, table_idx
                        )
                        
                        if len(table_text.strip()) < 20:
                            continue
                        
                        # –£–ª—É—á—à–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
                        improved_table_text = self.quality_improver.improve_text_quality(table_text)
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (—Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –Ω–∞—É—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã)
                        relevance = self._assess_table_relevance(improved_table_text)
                        
                        element = ExtractedElement(
                            element_type='table',
                            content=improved_table_text,
                            page_number=page_num,
                            confidence=relevance,
                            metadata={
                                'extraction_method': 'pdfplumber',
                                'table_index': table_idx,
                                'table_shape': f"{len(table)}x{len(table[0]) if table else 0}",
                                'relevance_score': relevance,
                                'contains_strain_data': 'strain' in improved_table_text.lower()
                            },
                            raw_data=table
                        )
                        
                        table_elements.append(element)
            
            logger.info(f"‚úÖ pdfplumber –∏–∑–≤–ª—ë–∫ {len(table_elements)} —Ç–∞–±–ª–∏—Ü")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ pdfplumber: {e}")
        
        return table_elements
    
    def _extract_tables_tabula(self, pdf_path: Path) -> List[ExtractedElement]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã —Å –ø–æ–º–æ—â—å—é tabula (–¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤)"""
        
        table_elements = []
        
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Ç–∞–±–ª–∏—Ü—ã —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            dfs = tabula.read_pdf(
                str(pdf_path), 
                pages='all', 
                multiple_tables=True,
                pandas_options={'header': 0}
            )
            
            for table_idx, df in enumerate(dfs):
                if df.empty or len(df) < 2:
                    continue
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º DataFrame –≤ —Ç–µ–∫—Å—Ç
                table_text = self._dataframe_to_structured_text(df, table_idx)
                
                if len(table_text.strip()) < 20:
                    continue
                
                # –£–ª—É—á—à–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
                improved_table_text = self.quality_improver.improve_text_quality(table_text)
                
                # –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                relevance = self._assess_table_relevance(improved_table_text)
                
                if relevance < 0.3:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
                    continue
                
                element = ExtractedElement(
                    element_type='table',
                    content=improved_table_text,
                    page_number=1,  # tabula –Ω–µ –¥–∞—ë—Ç —Ç–æ—á–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                    confidence=relevance,
                    metadata={
                        'extraction_method': 'tabula',
                        'table_index': table_idx,
                        'table_shape': f"{len(df)}x{len(df.columns)}",
                        'relevance_score': relevance,
                        'column_count': len(df.columns)
                    },
                    raw_data=df
                )
                
                table_elements.append(element)
            
            logger.info(f"‚úÖ tabula –∏–∑–≤–ª—ë–∫ {len(table_elements)} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è tabula –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –æ—à–∏–±–∫–∞: {e}")
        
        return table_elements
    
    def _table_to_structured_text(self, table: List[List], page_num: int, table_idx: int) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ç–∞–±–ª–∏—Ü—É –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"""
        
        if not table or len(table) < 2:
            return ""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        headers = [str(cell).strip() if cell else "" for cell in table[0]]
        headers = [h for h in headers if h]  # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ
        
        if not headers:
            return ""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        structured_text = f"–¢–ê–ë–õ–ò–¶–ê {table_idx + 1} (–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}):\n"
        structured_text += f"–°—Ç–æ–ª–±—Ü—ã: {' | '.join(headers)}\n\n"
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        for row_idx, row in enumerate(table[1:], 1):
            if not any(cell for cell in row):  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                continue
            
            row_data = [str(cell).strip() if cell else "-" for cell in row]
            
            # –°–æ–∑–¥–∞—ë–º —á–∏—Ç–∞–µ–º—É—é —Å—Ç—Ä–æ–∫—É
            row_text = ""
            for i, (header, value) in enumerate(zip(headers, row_data[:len(headers)])):
                if value and value != "-":
                    row_text += f"{header}: {value}; "
            
            if row_text:
                structured_text += f"–°—Ç—Ä–æ–∫–∞ {row_idx}: {row_text.rstrip('; ')}\n"
        
        return structured_text
    
    def _dataframe_to_structured_text(self, df: pd.DataFrame, table_idx: int) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç DataFrame –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"""
        
        structured_text = f"–¢–ê–ë–õ–ò–¶–ê {table_idx + 1} (tabula):\n"
        structured_text += f"–°—Ç–æ–ª–±—Ü—ã: {' | '.join(df.columns)}\n\n"
        
        for idx, row in df.iterrows():
            row_text = ""
            for col in df.columns:
                value = str(row[col]).strip()
                if value and value not in ['nan', 'NaN', '']:
                    row_text += f"{col}: {value}; "
            
            if row_text:
                structured_text += f"–°—Ç—Ä–æ–∫–∞ {idx + 1}: {row_text.rstrip('; ')}\n"
        
        return structured_text
    
    def _assess_table_relevance(self, table_text: str) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        
        table_lower = table_text.lower()
        relevance_score = 0.0
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –º–∏–∫—Ä–æ–±–∏–æ–ª–æ–≥–∏–∏
        microbiology_keywords = [
            'strain', '—à—Ç–∞–º–º', 'lysobacter', 'species', 'temperature', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞',
            'growth', '—Ä–æ—Å—Ç', 'ph', '—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏', 'characteristics',
            'biochemical', '–±–∏–æ—Ö–∏–º–∏—á–µ—Å–∫–∏–π', 'morphology', '–º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è',
            'gram', 'catalase', 'oxidase', 'glucose', '—Ä–∞–∑–º–µ—Ä', 'size'
        ]
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        for keyword in microbiology_keywords:
            if keyword in table_lower:
                relevance_score += 0.1
        
        # –ë–æ–Ω—É—Å –∑–∞ —à—Ç–∞–º–º—ã
        strain_patterns = [r'[A-Z]{1,3}[-\s]?\d{1,6}[A-Z]?', r'YC\d+', r'strain\s+\w+']
        for pattern in strain_patterns:
            if re.search(pattern, table_text, re.IGNORECASE):
                relevance_score += 0.2
        
        # –ë–æ–Ω—É—Å –∑–∞ —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã, pH –∏ —Ç.–¥.)
        numeric_patterns = [r'\d+[-‚Äì]\d+¬∞?[Cc–°]', r'pH\s*\d', r'\d+\.\d+']
        for pattern in numeric_patterns:
            if re.search(pattern, table_text):
                relevance_score += 0.1
        
        return min(1.0, relevance_score)
    
    def _classify_content(self, text: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['table', '—Ç–∞–±–ª–∏—Ü–∞', '|']):
            return 'table'
        elif any(word in text_lower for word in ['abstract', 'introduction', 'conclusion']):
            return 'section'
        elif len(text.split()) < 10:
            return 'header'
        else:
            return 'text'
    
    def _split_markdown_by_pages(self, markdown_text: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç markdown –Ω–∞ –ø—Ä–∏–º–µ—Ä–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å)
        avg_page_size = 2000  # —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
        pages = []
        
        current_page = ""
        for line in markdown_text.split('\n'):
            current_page += line + '\n'
            
            if len(current_page) > avg_page_size:
                pages.append(current_page)
                current_page = ""
        
        if current_page.strip():
            pages.append(current_page)
        
        return pages
    
    def _extract_with_basic_pymupdf(self, pdf_path: Path) -> List[ExtractedElement]:
        """Fallback –º–µ—Ç–æ–¥ —Å –±–∞–∑–æ–≤—ã–º PyMuPDF"""
        
        import fitz
        
        elements = []
        
        try:
            doc = fitz.open(pdf_path)
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                text = page.get_text()
                
                if text and len(text.strip()) > 20:
                    improved_text = self.quality_improver.improve_text_quality(text)
                    
                    element = ExtractedElement(
                        element_type='text',
                        content=improved_text,
                        page_number=page_num + 1,
                        confidence=0.7,  # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                        metadata={
                            'extraction_method': 'basic_pymupdf',
                            'fallback': True
                        }
                    )
                    
                    elements.append(element)
            
            doc.close()
            
        except Exception as e:
            logger.error(f"‚ùå –î–∞–∂–µ fallback –º–µ—Ç–æ–¥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
        
        return elements
    
    def _create_fallback_document(self, pdf_path: Path, error_msg: str) -> AdvancedExtractedDocument:
        """–°–æ–∑–¥–∞—ë—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        
        return AdvancedExtractedDocument(
            file_path=str(pdf_path),
            title=pdf_path.stem,
            total_pages=0,
            elements=[],
            metadata={
                'error': error_msg,
                'extraction_failed': True
            },
            extraction_stats={
                'total_pages': 0,
                'text_elements': 0,
                'table_elements': 0,
                'quality_score': 0,
                'methods_used': []
            }
        ) 