#!/usr/bin/env python3
"""
–¢–ï–°–¢: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —É–º–Ω–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–º
"""

import sys
import os
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
sys.path.insert(0, str(Path(__file__).parent / "src"))
sys.path.insert(0, str(Path(__file__).parent))

from lysobacter_rag.pdf_extractor.advanced_pdf_extractor import AdvancedPDFExtractor

def test_smart_chunking():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —É–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º PDF"""
    
    print("üß¨ –¢–ï–°–¢ –£–ú–ù–û–ì–û –ß–ê–ù–ö–ò–ù–ì–ê")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å —É–º–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º
    extractor = AdvancedPDFExtractor(use_smart_chunking=True)
    
    # –ù–∞—Ö–æ–¥–∏–º —Ç–µ—Å—Ç–æ–≤—ã–π PDF
    data_dir = Path("data")
    pdf_files = list(data_dir.glob("*.pdf"))
    
    if not pdf_files:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã PDF —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ data/")
        return
    
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π PDF –¥–ª—è —Ç–µ—Å—Ç–∞
    test_pdf = pdf_files[0]
    print(f"üìÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ —Ñ–∞–π–ª–µ: {test_pdf.name}")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
    print("\nüöÄ –®–∞–≥ 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–º")
    document = extractor.extract_document(test_pdf)
    
    print(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(document.elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
    print(f"   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"      - –°—Ç—Ä–∞–Ω–∏—Ü: {document.total_pages}")
    print(f"      - –¢–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {document.extraction_stats.get('text_elements', 0)}")
    print(f"      - –¢–∞–±–ª–∏—á–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {document.extraction_stats.get('table_elements', 0)}")
    print(f"      - –ö–∞—á–µ—Å—Ç–≤–æ: {document.extraction_stats.get('quality_score', 0):.1f}%")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —É–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥
    print("\nüß¨ –®–∞–≥ 2: –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —É–º–Ω–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞")
    smart_chunks = extractor.get_smart_chunks(document)
    
    print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(smart_chunks)} —É–º–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\nüìä –®–∞–≥ 3: –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —É–º–Ω–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞")
    
    total_length = sum(len(chunk['content']) for chunk in smart_chunks)
    avg_length = total_length / len(smart_chunks) if smart_chunks else 0
    
    print(f"   üìà –û–±—â–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {total_length:,} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   üìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞: {avg_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
    chunk_types = {}
    importance_levels = {}
    
    for chunk in smart_chunks:
        chunk_type = chunk['metadata'].get('chunk_type', 'unknown')
        importance = chunk['metadata'].get('scientific_importance', 'unknown')
        
        chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
        importance_levels[importance] = importance_levels.get(importance, 0) + 1
    
    print(f"\n   üìã –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º:")
    for chunk_type, count in chunk_types.items():
        print(f"      - {chunk_type}: {count}")
    
    print(f"\n   üéØ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏:")
    for importance, count in importance_levels.items():
        print(f"      - {importance}: {count}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤
    print("\nüîç –®–∞–≥ 4: –ü—Ä–∏–º–µ—Ä—ã —É–º–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
    
    # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ —á–∞–Ω–∫–∏
    important_chunks = [c for c in smart_chunks 
                       if c['metadata'].get('scientific_importance') in ['critical', 'high']]
    
    if important_chunks:
        print(f"\n   üéØ –í–∞–∂–Ω—ã–µ —á–∞–Ω–∫–∏ ({len(important_chunks)} –∏–∑ {len(smart_chunks)}):")
        
        for i, chunk in enumerate(important_chunks[:3], 1):
            importance = chunk['metadata'].get('scientific_importance', 'unknown')
            key_terms = chunk['metadata'].get('key_terms', [])
            chunk_type = chunk['metadata'].get('chunk_type', 'unknown')
            
            print(f"\n   üî∏ –ß–∞–Ω–∫ {i} ({chunk_type}, {importance}):")
            print(f"      –ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: {', '.join(key_terms[:5]) if key_terms else '–Ω–µ—Ç'}")
            print(f"      –î–ª–∏–Ω–∞: {len(chunk['content'])} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"      –¢–µ–∫—Å—Ç: {chunk['content'][:150]}...")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–æ—Å—Ç—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º
    print("\n‚öñÔ∏è –®–∞–≥ 5: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–æ—Å—Ç—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º")
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    simple_extractor = AdvancedPDFExtractor(use_smart_chunking=False)
    simple_document = simple_extractor.extract_document(test_pdf)
    simple_chunks = simple_extractor.get_smart_chunks(simple_document)
    
    simple_avg_length = sum(len(c['content']) for c in simple_chunks) / len(simple_chunks) if simple_chunks else 0
    
    print(f"   üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ:")
    print(f"      –£–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥:    {len(smart_chunks)} —á–∞–Ω–∫–æ–≤, —Å—Ä. –¥–ª–∏–Ω–∞ {avg_length:.0f}")
    print(f"      –ü—Ä–æ—Å—Ç–æ–π —á–∞–Ω–∫–∏–Ω–≥:  {len(simple_chunks)} —á–∞–Ω–∫–æ–≤, —Å—Ä. –¥–ª–∏–Ω–∞ {simple_avg_length:.0f}")
    
    improvement_factor = len(simple_chunks) / len(smart_chunks) if smart_chunks else 1
    print(f"      –°–∂–∞—Ç–∏–µ –≤ {improvement_factor:.1f}x —Ä–∞–∑")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
    print("\nüî¨ –®–∞–≥ 6: –ê–Ω–∞–ª–∏–∑ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤")
    
    all_key_terms = []
    for chunk in smart_chunks:
        terms = chunk['metadata'].get('key_terms', [])
        all_key_terms.extend(terms)
    
    unique_terms = list(set(all_key_terms))
    scientific_terms = [term for term in unique_terms 
                       if any(sci_word in term.lower() 
                             for sci_word in ['lysobacter', 'strain', 'ph', '¬∞c', 'rna', 'catalase'])]
    
    print(f"   üß¨ –í—Å–µ–≥–æ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(all_key_terms)}")
    print(f"   üéØ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(unique_terms)}")
    print(f"   üî¨ –ù–∞—É—á–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(scientific_terms)}")
    
    if scientific_terms:
        print(f"   üìù –ü—Ä–∏–º–µ—Ä—ã –Ω–∞—É—á–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {', '.join(scientific_terms[:10])}")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    print(f"\n‚úÖ –ò–¢–û–ì: –£–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω—ë–Ω!")
    print(f"   - –°–æ–∑–¥–∞–Ω–æ {len(smart_chunks)} –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
    print(f"   - –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {avg_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤ (—Ü–µ–ª—å: 350)")
    print(f"   - –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(scientific_terms)} –Ω–∞—É—á–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤")
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    size_score = max(0, 100 - abs(avg_length - 350) / 350 * 100)
    term_score = min(100, len(scientific_terms) * 10)
    overall_score = (size_score + term_score) / 2
    
    print(f"   üìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {overall_score:.1f}%")
    
    if overall_score >= 80:
        print("   üèÜ –û–¢–õ–ò–ß–ù–û–ï –∫–∞—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–∏–Ω–≥–∞!")
    elif overall_score >= 60:
        print("   ‚úÖ –•–û–†–û–®–ï–ï –∫–∞—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–∏–Ω–≥–∞")
    else:
        print("   ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–∏–µ")

if __name__ == "__main__":
    test_smart_chunking() 