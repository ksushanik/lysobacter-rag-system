#!/usr/bin/env python3
"""
–¢–ï–°–¢ –ö–ê–ß–ï–°–¢–í–ê: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –¥–æ –∏ –ø–æ—Å–ª–µ —É–º–Ω–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞
"""

import sys
import os
from pathlib import Path
import time

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
sys.path.insert(0, str(Path(__file__).parent / "src"))
sys.path.insert(0, str(Path(__file__).parent))

from lysobacter_rag.indexer.indexer import Indexer

def test_quality_improvements():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ—Å–ª–µ —É–º–Ω–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞"""
    
    print("üî¨ –¢–ï–°–¢ –ö–ê–ß–ï–°–¢–í–ê: –î–æ vs –ü–æ—Å–ª–µ —É–º–Ω–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—É
    indexer = Indexer()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –±–∞–∑—ã
    stats = indexer.get_collection_stats()
    print(f"üìä –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –±–∞–∑—ã:")
    print(f"   –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {stats.get('total_chunks', 0)}")
    
    if stats.get('total_chunks', 0) == 0:
        print("‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç–∞! –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —Å–Ω–∞—á–∞–ª–∞.")
        return False
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    test_cases = [
        {
            'query': 'Lysobacter capsici YC5194 —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏',
            'expected_keywords': ['YC5194', 'capsici', 'type strain', 'rhizosphere', 'pepper'],
            'description': '–û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —à—Ç–∞–º–º–µ YC5194'
        },
        {
            'query': 'temperature range growth Lysobacter',
            'expected_keywords': ['temperature', '¬∞C', 'growth', 'range', 'optimal'],
            'description': '–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ —É—Å–ª–æ–≤–∏—è —Ä–æ—Å—Ç–∞'
        },
        {
            'query': 'G+C content DNA mol%',
            'expected_keywords': ['G+C', 'content', 'mol%', 'DNA', 'genomic'],
            'description': '–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ G+C –≤ –î–ù–ö'
        },
        {
            'query': 'cell morphology size micrometers',
            'expected_keywords': ['cell', 'morphology', 'Œºm', 'size', 'rod-shaped'],
            'description': '–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è –∫–ª–µ—Ç–æ–∫'
        },
        {
            'query': 'pH range tolerance acidic alkaline',
            'expected_keywords': ['pH', 'range', 'growth', 'acidic', 'alkaline'],
            'description': 'pH —Ç–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç—å'
        },
        {
            'query': 'catalase oxidase positive biochemical',
            'expected_keywords': ['catalase', 'oxidase', 'positive', 'biochemical', 'enzyme'],
            'description': '–ë–∏–æ—Ö–∏–º–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏'
        },
        {
            'query': '16S rRNA phylogenetic analysis',
            'expected_keywords': ['16S', 'rRNA', 'phylogenetic', 'sequence', 'analysis'],
            'description': '–§–∏–ª–æ–≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑'
        },
        {
            'query': 'antimicrobial activity biocontrol plant pathogen',
            'expected_keywords': ['antimicrobial', 'activity', 'biocontrol', 'pathogen', 'plant'],
            'description': '–ê–Ω—Ç–∏–º–∏–∫—Ä–æ–±–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å'
        }
    ]
    
    print(f"\nüîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {len(test_cases)} –∑–∞–ø—Ä–æ—Å–æ–≤...")
    print("-" * 50)
    
    total_score = 0
    detailed_results = []
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{i}. üîç {test_case['description']}")
        print(f"   –ó–∞–ø—Ä–æ—Å: '{test_case['query']}'")
        
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            results = indexer.search(test_case['query'], top_k=5)
            
            if not results:
                print(f"   ‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                detailed_results.append({
                    'query': test_case['query'],
                    'description': test_case['description'],
                    'score': 0,
                    'issues': ['no_results'],
                    'relevance_scores': []
                })
                continue
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            relevance_scores = [r['relevance_score'] for r in results]
            avg_relevance = sum(relevance_scores) / len(relevance_scores)
            max_relevance = max(relevance_scores)
            
            print(f"   üìä –ù–∞–π–¥–µ–Ω–æ: {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            print(f"   üìà –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: —Å—Ä={avg_relevance:.3f}, –º–∞–∫—Å={max_relevance:.3f}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–∂–∏–¥–∞–µ–º—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            all_text = ' '.join([r['text'] for r in results])
            found_keywords = []
            for keyword in test_case['expected_keywords']:
                if keyword.lower() in all_text.lower():
                    found_keywords.append(keyword)
            
            keyword_ratio = len(found_keywords) / len(test_case['expected_keywords'])
            print(f"   üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {len(found_keywords)}/{len(test_case['expected_keywords'])} ({keyword_ratio:.1%})")
            print(f"      –ù–∞–π–¥–µ–Ω—ã: {', '.join(found_keywords) if found_keywords else '–Ω–µ—Ç'}")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤
            chunk_sizes = [len(r['text']) for r in results]
            avg_chunk_size = sum(chunk_sizes) / len(chunk_sizes)
            print(f"   üìè –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {avg_chunk_size:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            sources = set(r['metadata'].get('source_pdf', 'unknown') for r in results)
            print(f"   üìö –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(sources)}")
            
            # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–π –±–∞–ª–ª
            relevance_score = min(100, avg_relevance * 100)
            keyword_score = keyword_ratio * 100
            size_score = max(0, 100 - abs(avg_chunk_size - 350) / 350 * 100)  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä 350
            diversity_score = min(100, len(sources) * 25)  # –ú–∞–∫—Å–∏–º—É–º 4 –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            
            overall_score = (relevance_score + keyword_score + size_score + diversity_score) / 4
            
            print(f"   üéØ –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {overall_score:.1f}%")
            print(f"      –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {relevance_score:.1f}%")
            print(f"      –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keyword_score:.1f}%")
            print(f"      –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤: {size_score:.1f}%")
            print(f"      –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: {diversity_score:.1f}%")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã
            issues = []
            if avg_relevance < 0.3:
                issues.append('low_relevance')
            if keyword_ratio < 0.5:
                issues.append('missing_keywords')
            if avg_chunk_size > 1000:
                issues.append('chunks_too_large')
            if avg_chunk_size < 100:
                issues.append('chunks_too_small')
            if len(sources) < 2:
                issues.append('low_diversity')
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            best_result = results[0]
            print(f"   üí° –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å {best_result['relevance_score']:.3f}):")
            print(f"      –ò—Å—Ç–æ—á–Ω–∏–∫: {best_result['metadata'].get('source_pdf', 'N/A')}")
            print(f"      –¢–µ–∫—Å—Ç: {best_result['text'][:200]}...")
            
            detailed_results.append({
                'query': test_case['query'],
                'description': test_case['description'],
                'score': overall_score,
                'relevance_scores': relevance_scores,
                'keyword_ratio': keyword_ratio,
                'chunk_size': avg_chunk_size,
                'source_count': len(sources),
                'issues': issues
            })
            
            total_score += overall_score
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
            detailed_results.append({
                'query': test_case['query'],
                'description': test_case['description'],
                'score': 0,
                'issues': [f'error: {str(e)}'],
                'relevance_scores': []
            })
    
    # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
    avg_score = total_score / len(test_cases) if test_cases else 0
    
    print(f"\nüèÜ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print("=" * 40)
    print(f"üìä –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {avg_score:.1f}%")
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    excellent_tests = [r for r in detailed_results if r['score'] >= 80]
    good_tests = [r for r in detailed_results if 60 <= r['score'] < 80]
    poor_tests = [r for r in detailed_results if r['score'] < 60]
    
    print(f"üèÜ –û—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(excellent_tests)}/{len(test_cases)}")
    print(f"‚úÖ –•–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(good_tests)}/{len(test_cases)}")
    print(f"‚ö†Ô∏è –ü–ª–æ—Ö–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(poor_tests)}/{len(test_cases)}")
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º
    all_issues = []
    for result in detailed_results:
        all_issues.extend(result['issues'])
    
    issue_counts = {}
    for issue in all_issues:
        issue_counts[issue] = issue_counts.get(issue, 0) + 1
    
    if issue_counts:
        print(f"\nüîç –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:")
        for issue, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"   - {issue}: {count} —Å–ª—É—á–∞–µ–≤")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    if avg_score >= 80:
        print(f"\nüéâ –û–¢–õ–ò–ß–ù–û: –£–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª –∫–∞—á–µ—Å—Ç–≤–æ!")
        print(f"   –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è")
        success_level = "excellent"
    elif avg_score >= 60:
        print(f"\n‚úÖ –•–û–†–û–®–û: –ó–∞–º–µ—Ç–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞")
        print(f"   –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∏–µ–º–ª–µ–º–æ, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å")
        success_level = "good"
    elif avg_score >= 40:
        print(f"\n‚ö†Ô∏è –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û: –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —É–ª—É—á—à–µ–Ω–∏—è")
        print(f"   –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞")
        success_level = "satisfactory"
    else:
        print(f"\n‚ùå –ü–õ–û–•–û: –°–µ—Ä—å—ë–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –æ—Å—Ç–∞—é—Ç—Å—è")
        print(f"   –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å –ø–æ–¥—Ö–æ–¥")
        success_level = "poor"
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    
    if 'low_relevance' in issue_counts:
        print("   üîß –£–ª—É—á—à–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–ª–∏ –º–æ–¥–µ–ª—å")
    if 'missing_keywords' in issue_counts:
        print("   üîß –£–ª—É—á—à–∏—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤")
    if 'chunks_too_large' in issue_counts:
        print("   üîß –£–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤")
    if 'chunks_too_small' in issue_counts:
        print("   üîß –£–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤")
    if 'low_diversity' in issue_counts:
        print("   üîß –£–ª—É—á—à–∏—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∏—Å–∫–∞ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    print(f"\nüìà –ü–†–û–ì–†–ï–°–° –ü–û –°–†–ê–í–ù–ï–ù–ò–Æ –° –î–ò–ê–ì–ù–û–°–¢–ò–ö–û–ô:")
    print("   –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø–æ–∫–∞–∑–∞–ª–∞ —Å—Ä–µ–¥–Ω—é—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: 0.444")
    if detailed_results:
        current_avg_relevance = sum(
            sum(r['relevance_scores']) / len(r['relevance_scores']) 
            for r in detailed_results 
            if r['relevance_scores']
        ) / len([r for r in detailed_results if r['relevance_scores']])
        
        improvement = (current_avg_relevance - 0.444) / 0.444 * 100
        print(f"   –¢–µ–∫—É—â–∞—è —Å—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {current_avg_relevance:.3f}")
        print(f"   –£–ª—É—á—à–µ–Ω–∏–µ: {improvement:+.1f}%")
    
    return success_level in ['excellent', 'good']

if __name__ == "__main__":
    success = test_quality_improvements()
    exit(0 if success else 1) 