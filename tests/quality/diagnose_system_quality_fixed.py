#!/usr/bin/env python3
"""
–°–ò–°–¢–ï–ú–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ –≤—Å–µ—Ö PDF —Ñ–∞–π–ª–æ–≤
"""

import sys
import os
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
sys.path.insert(0, str(Path(__file__).parent / "src"))
sys.path.insert(0, str(Path(__file__).parent))

from lysobacter_rag.indexer.indexer import Indexer
from config import config

print("üîç –°–ò–°–¢–ï–ú–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø")
print("=" * 60)

indexer = Indexer()
stats = indexer.get_collection_stats()

print(f"üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
print(f"   –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {stats.get('total_chunks', 0)}")

# –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
collection = indexer.chroma_client.get_collection(name=config.CHROMA_COLLECTION_NAME)
all_data = collection.get()
all_metadata = all_data['metadatas']
all_documents = all_data['documents']

print(f"   –ü–æ–ª—É—á–µ–Ω–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {len(all_metadata)}")

# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
sources = {}
total_text_length = 0
total_documents = len(all_documents)

for i, (metadata, document) in enumerate(zip(all_metadata, all_documents)):
    source = metadata.get('source_pdf', 'unknown')
    chunk_type = metadata.get('chunk_type', 'text')
    
    if source not in sources:
        sources[source] = {'text': 0, 'table': 0, 'total': 0, 'avg_length': 0, 'total_length': 0}
    
    sources[source][chunk_type] = sources[source].get(chunk_type, 0) + 1
    sources[source]['total'] += 1
    sources[source]['total_length'] += len(document) if document else 0
    
    total_text_length += len(document) if document else 0

# –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –¥–ª–∏–Ω—ã
for source_data in sources.values():
    if source_data['total'] > 0:
        source_data['avg_length'] = source_data['total_length'] // source_data['total']

print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞: {total_text_length // total_documents if total_documents > 0 else 0} —Å–∏–º–≤–æ–ª–æ–≤")

print(f"\nüìö –ê–ù–ê–õ–ò–ó –ü–û –ò–°–¢–û–ß–ù–ò–ö–ê–ú (—Ç–æ–ø-20):")
print("-" * 80)

# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ–±—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —á–∞–Ω–∫–æ–≤
sorted_sources = sorted(sources.items(), key=lambda x: x[1]['total'], reverse=True)

print(f"{'–§–∞–π–ª':<35} {'–¢–µ–∫—Å—Ç':<6} {'–¢–∞–±–ª–∏—Ü—ã':<8} {'–í—Å–µ–≥–æ':<6} {'–°—Ä.–¥–ª–∏–Ω–∞':<8}")
print("-" * 80)

problematic_sources = []
good_sources = []

for source, counts in sorted_sources[:20]:
    filename = source.replace('.pdf', '')[:30] + '...' if len(source) > 33 else source
    
    avg_len = counts['avg_length']
    total = counts['total']
    
    print(f"{filename:<35} {counts.get('text', 0):<6} {counts.get('table', 0):<8} {total:<6} {avg_len:<8}")
    
    # –í—ã—è–≤–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    if total < 3:  # –°–ª–∏—à–∫–æ–º –º–∞–ª–æ —á–∞–Ω–∫–æ–≤
        problematic_sources.append((source, "–º–∞–ª–æ —á–∞–Ω–∫–æ–≤"))
    elif avg_len < 50:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —á–∞–Ω–∫–∏
        problematic_sources.append((source, "–∫–æ—Ä–æ—Ç–∫–∏–µ —á–∞–Ω–∫–∏"))
    elif avg_len > 2000:  # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
        problematic_sources.append((source, "–¥–ª–∏–Ω–Ω—ã–µ —á–∞–Ω–∫–∏"))
    else:
        good_sources.append(source)

print(f"\nüéØ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –ò–°–¢–û–ß–ù–ò–ö–û–í:")
print("-" * 40)
print(f"‚úÖ –•–æ—Ä–æ—à–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: {len(good_sources)}")
print(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: {len(problematic_sources)}")

if problematic_sources:
    print(f"\n‚ùå –ü–†–û–ë–õ–ï–ú–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò:")
    for source, issue in problematic_sources[:10]:
        filename = source.replace('.pdf', '')[:40]
        print(f"   - {filename}: {issue}")

# –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –Ω–∞—É—á–Ω—ã–º —Ç–µ—Ä–º–∏–Ω–∞–º
print(f"\nüß¨ –¢–ï–°–¢ –ü–û–ò–°–ö–ê –ù–ê–£–ß–ù–´–• –¢–ï–†–ú–ò–ù–û–í:")
print("-" * 50)

scientific_terms = [
    "Lysobacter",
    "type strain", 
    "sp. nov.",
    "16S rRNA",
    "phylogenetic",
    "G+C content",
    "temperature range",
    "pH range",
    "catalase positive",
    "cell morphology"
]

search_quality = []

for term in scientific_terms:
    try:
        results = indexer.search(term, top_k=5)
        
        if results:
            avg_relevance = sum(r['relevance_score'] for r in results) / len(results)
            best_relevance = max(r['relevance_score'] for r in results)
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            unique_sources = set(r['metadata'].get('source_pdf', '') for r in results)
            
            search_quality.append({
                'term': term,
                'avg_relevance': avg_relevance,
                'best_relevance': best_relevance,
                'results_count': len(results),
                'unique_sources': len(unique_sources)
            })
            
            status = "‚úÖ" if avg_relevance > 0.4 else "‚ö†Ô∏è" if avg_relevance > 0.2 else "‚ùå"
            print(f"{status} {term:<18} - —Å—Ä:{avg_relevance:.3f} –º–∞–∫—Å:{best_relevance:.3f} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:{len(unique_sources)}")
        else:
            print(f"‚ùå {term:<18} - –ù–ï –ù–ê–ô–î–ï–ù")
            search_quality.append({
                'term': term,
                'avg_relevance': 0,
                'best_relevance': 0,
                'results_count': 0,
                'unique_sources': 0
            })
    except Exception as e:
        print(f"‚ùå {term:<18} - –û–®–ò–ë–ö–ê: {e}")

# –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏—Å—Ç–µ–º—ã
avg_search_quality = sum(sq['avg_relevance'] for sq in search_quality) / len(search_quality)
terms_found = sum(1 for sq in search_quality if sq['results_count'] > 0)

print(f"\nüìä –û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –°–ò–°–¢–ï–ú–´:")
print("-" * 40)
print(f"üìà –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞: {avg_search_quality:.3f}")
print(f"üéØ –ù–∞–π–¥–µ–Ω–æ —Ç–µ—Ä–º–∏–Ω–æ–≤: {terms_found}/{len(scientific_terms)} ({terms_found/len(scientific_terms)*100:.1f}%)")
print(f"üìö –í—Å–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(sources)}")
print(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(problematic_sources)} ({len(problematic_sources)/len(sources)*100:.1f}%)")

# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ:")
print("-" * 40)

if avg_search_quality < 0.3:
    print("üö® –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï: –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞")
    print("   ‚û§ –¢—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–º")
elif avg_search_quality < 0.5:
    print("‚ö†Ô∏è –°–ï–†–¨–ï–ó–ù–û: –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞")
    print("   ‚û§ –£–ª—É—á—à–∏—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –ø—Ä–æ–º–ø—Ç—ã")
else:
    print("‚úÖ –•–û–†–û–®–û: –ü—Ä–∏–µ–º–ª–µ–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞")

if len(problematic_sources) > len(sources) * 0.3:
    print("üìâ –ú–Ω–æ–≥–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    print("   ‚û§ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —á–∞–Ω–∫–∏–Ω–≥–∞")

# –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π
print(f"\nüöÄ –ü–õ–ê–ù –î–ï–ô–°–¢–í–ò–ô:")
print("1. –°–æ–∑–¥–∞—Ç—å —É–ª—É—á—à–µ–Ω–Ω—ã–π PDF —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å –ª—É—á—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –Ω–∞—É—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤")
print("2. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π: 300-600 —Å–∏–º–≤–æ–ª–æ–≤)")
print("3. –î–æ–±–∞–≤–∏—Ç—å –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è OCR –æ—à–∏–±–æ–∫")
print("4. –í–Ω–µ–¥—Ä–∏—Ç—å –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π + —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)")
print("5. –£–ª—É—á—à–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã RAG –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")

if problematic_sources:
    print(f"\nüéØ –ü–†–ò–û–†–ò–¢–ï–¢–ù–´–ï –§–ê–ô–õ–´ –î–õ–Ø –ü–ï–†–ï–ò–ù–î–ï–ö–°–ê–¶–ò–ò:")
    for source, issue in problematic_sources[:5]:
        print(f"   - {source}")

print(f"\n‚úÖ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê") 