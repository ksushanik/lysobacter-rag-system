#!/usr/bin/env python3
"""
üèÜ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –±–µ–Ω—á–º–∞—Ä–∫ LLM –º–æ–¥–µ–ª–µ–π –¥–ª—è RAG-—Å–∏—Å—Ç–µ–º—ã –ª–∏–∑–æ–±–∞–∫—Ç–µ—Ä–∏–π
================================================================

–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏.
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.
"""
import sys
import time
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))
sys.path.insert(0, str(project_root / "scripts"))

from lysobacter_rag.rag_pipeline.rag_pipeline import RAGPipeline
from config import config

class ModelBenchmark:
    """–ö–ª–∞—Å—Å –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.models = [
            "deepseek/deepseek-chat",                # –ë–∞–∑–æ–≤–∞—è DeepSeek
            "deepseek/deepseek-v3-base:free",        # –ù–æ–≤–∞—è v3 base
            "deepseek/deepseek-chat-v3-0324:free",   # –ß–∞—Ç v3
            "deepseek/deepseek-r1:free",             # R1 –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
            "google/gemini-2.0-flash-exp:free"       # Google Gemini 2.0
        ]
        
        self.test_queries = [
            "–ß—Ç–æ –∏–∑–≤–µ—Å—Ç–Ω–æ –æ —à—Ç–∞–º–º–µ GW1-59T?",
            "GW1-59T strain characteristics",
            "–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è –ª–∏–∑–æ–±–∞–∫—Ç–µ—Ä–∏–π GW1-59T",
            "–ì–¥–µ –±—ã–ª –≤—ã–¥–µ–ª–µ–Ω —à—Ç–∞–º–º GW1-59T?",
            "–ë–∏–æ—Ö–∏–º–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞ GW1-59T"
        ]
        
        self.results_file = project_root / "benchmark_results.json"
        
    def progress_bar(self, current: int, total: int, width: int = 40) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        percent = current / total
        filled = int(width * percent)
        bar = "‚ñà" * filled + "‚ñë" * (width - filled)
        return f"[{bar}] {percent:.1%} ({current}/{total})"
    
    def test_single_model(self, model: str, query: str) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–¥–Ω—É –º–æ–¥–µ–ª—å –Ω–∞ –æ–¥–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ"""
        print(f"  üî¨ –ú–æ–¥–µ–ª—å: {model}")
        print(f"  üìù –ó–∞–ø—Ä–æ—Å: {query}")
        
        # –ú–µ–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ –∫–æ–Ω—Ñ–∏–≥–µ
        original_model = config.OPENROUTER_MODEL
        config.OPENROUTER_MODEL = model
        
        try:
            # –°–æ–∑–¥–∞–µ–º RAG pipeline
            rag = RAGPipeline()
            
            start_time = time.time()
            result = rag.ask_question(query)
            end_time = time.time()
            
            response_time = end_time - start_time
            answer = result.get('answer', '')
            answer_length = len(answer)
            sources_count = len(result.get('sources', []))
            confidence = result.get('confidence', 0)
            
            print(f"  ‚úÖ –í—Ä–µ–º—è: {response_time:.2f}—Å | –°–∏–º–≤–æ–ª–æ–≤: {answer_length} | –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {sources_count}")
            
            return {
                'model': model,
                'query': query,
                'success': True,
                'response_time': response_time,
                'answer_length': answer_length,
                'sources_count': sources_count,
                'confidence': confidence,
                'answer': answer,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            error_msg = str(e)
            print(f"  ‚ùå –û—à–∏–±–∫–∞: {error_msg[:50]}...")
            
            return {
                'model': model,
                'query': query,
                'success': False,
                'error': error_msg,
                'response_time': 0,
                'answer_length': 0,
                'sources_count': 0,
                'confidence': 0,
                'timestamp': datetime.now().isoformat()
            }
        
        finally:
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
            config.OPENROUTER_MODEL = original_model
    
    def run_full_benchmark(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö"""
        print("üèÜ –ü–û–õ–ù–´–ô –ë–ï–ù–ß–ú–ê–†–ö LLM –ú–û–î–ï–õ–ï–ô")
        print("=" * 60)
        print(f"üìä –ú–æ–¥–µ–ª–µ–π: {len(self.models)}")
        print(f"üìù –ó–∞–ø—Ä–æ—Å–æ–≤: {len(self.test_queries)}")
        print(f"üî¢ –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {len(self.models) * len(self.test_queries)}")
        print("=" * 60)
        
        all_results = []
        total_tests = len(self.models) * len(self.test_queries)
        current_test = 0
        
        benchmark_start = datetime.now()
        
        for model_idx, model in enumerate(self.models, 1):
            print(f"\nüéØ [{model_idx}/{len(self.models)}] –¢–µ—Å—Ç–∏—Ä—É—é –º–æ–¥–µ–ª—å: {model}")
            print("-" * 50)
            
            model_results = []
            
            for query_idx, query in enumerate(self.test_queries, 1):
                current_test += 1
                
                print(f"\n{self.progress_bar(current_test, total_tests)}")
                print(f"üìç –¢–µ—Å—Ç {current_test}/{total_tests}")
                
                result = self.test_single_model(model, query)
                model_results.append(result)
                all_results.append(result)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                self.save_results({
                    'benchmark_start': benchmark_start.isoformat(),
                    'current_time': datetime.now().isoformat(),
                    'progress': f"{current_test}/{total_tests}",
                    'results': all_results
                })
                
                time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–æ–¥–µ–ª–∏
            successful = [r for r in model_results if r['success']]
            if successful:
                avg_time = sum(r['response_time'] for r in successful) / len(successful)
                avg_length = sum(r['answer_length'] for r in successful) / len(successful)
                success_rate = len(successful) / len(model_results) * 100
                
                print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏ {model}:")
                print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1f}%")
                print(f"   ‚è±Ô∏è –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.2f}—Å")
                print(f"   üìù –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        
        benchmark_end = datetime.now()
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        final_results = {
            'benchmark_info': {
                'start_time': benchmark_start.isoformat(),
                'end_time': benchmark_end.isoformat(),
                'duration_seconds': (benchmark_end - benchmark_start).total_seconds(),
                'total_tests': total_tests,
                'models_tested': len(self.models),
                'queries_tested': len(self.test_queries)
            },
            'results': all_results,
            'summary': self.analyze_results(all_results)
        }
        
        self.save_results(final_results)
        self.print_summary(final_results)
        
        return final_results
    
    def analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        successful = [r for r in results if r['success']]
        failed = [r for r in results if not r['success']]
        
        if not successful:
            return {'error': '–ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –º–æ–¥–µ–ª—è–º
        by_model = {}
        for result in successful:
            model = result['model']
            if model not in by_model:
                by_model[model] = []
            by_model[model].append(result)
        
        model_stats = {}
        for model, model_results in by_model.items():
            model_stats[model] = {
                'success_rate': len(model_results) / len([r for r in results if r['model'] == model]) * 100,
                'avg_response_time': sum(r['response_time'] for r in model_results) / len(model_results),
                'avg_answer_length': sum(r['answer_length'] for r in model_results) / len(model_results),
                'avg_sources_count': sum(r['sources_count'] for r in model_results) / len(model_results),
                'total_tests': len(model_results)
            }
        
        # –†–µ–π—Ç–∏–Ω–≥–∏
        fastest_model = min(model_stats.items(), key=lambda x: x[1]['avg_response_time'])
        most_detailed = max(model_stats.items(), key=lambda x: x[1]['avg_answer_length'])
        most_sources = max(model_stats.items(), key=lambda x: x[1]['avg_sources_count'])
        
        return {
            'total_successful': len(successful),
            'total_failed': len(failed),
            'success_rate_overall': len(successful) / len(results) * 100,
            'model_statistics': model_stats,
            'rankings': {
                'fastest': {'model': fastest_model[0], 'time': fastest_model[1]['avg_response_time']},
                'most_detailed': {'model': most_detailed[0], 'length': most_detailed[1]['avg_answer_length']},
                'most_sources': {'model': most_sources[0], 'sources': most_sources[1]['avg_sources_count']}
            }
        }
    
    def save_results(self, results: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON —Ñ–∞–π–ª"""
        with open(self.results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
    
    def print_summary(self, results: Dict[str, Any]):
        """–í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        summary = results['summary']
        
        print(f"\nüèÜ –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ë–ï–ù–ß–ú–ê–†–ö–ê")
        print("=" * 60)
        print(f"‚è±Ô∏è –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {results['benchmark_info']['duration_seconds']:.1f} —Å–µ–∫—É–Ω–¥")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {summary['total_successful']}/{results['benchmark_info']['total_tests']}")
        print(f"üìä –û–±—â–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {summary['success_rate_overall']:.1f}%")
        
        print(f"\nüèÖ –†–ï–ô–¢–ò–ù–ì–ò:")
        print(f"üöÄ –°–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è: {summary['rankings']['fastest']['model']} ({summary['rankings']['fastest']['time']:.2f}—Å)")
        print(f"üìù –°–∞–º—ã–µ –ø–æ–¥—Ä–æ–±–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã: {summary['rankings']['most_detailed']['model']} ({summary['rankings']['most_detailed']['length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤)")
        print(f"üìö –ë–æ–ª—å—à–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {summary['rankings']['most_sources']['model']} ({summary['rankings']['most_sources']['sources']:.1f} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤)")
        
        print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ú–û–î–ï–õ–Ø–ú:")
        for model, stats in summary['model_statistics'].items():
            print(f"   {model}:")
            print(f"      ‚úÖ –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {stats['success_rate']:.1f}%")
            print(f"      ‚è±Ô∏è –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {stats['avg_response_time']:.2f}—Å")
            print(f"      üìù –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {stats['avg_answer_length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"      üìö –°—Ä–µ–¥–Ω–µ–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {stats['avg_sources_count']:.1f}")
        
        print(f"\nüíæ –ü–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.results_file}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    benchmark = ModelBenchmark()
    
    print("üöÄ –ó–∞–ø—É—Å–∫–∞—é –ø–æ–ª–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –º–æ–¥–µ–ª–µ–π...")
    print("üí° –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ benchmark_results.json")
    print("‚èØÔ∏è –ü—Ä–æ–≥—Ä–µ—Å—Å –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏")
    
    input("\nüìç –ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è...")
    
    try:
        results = benchmark.run_full_benchmark()
        print(f"\nüéâ –ë–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        
    except KeyboardInterrupt:
        print(f"\n‚è∏Ô∏è –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        print(f"üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {benchmark.results_file}")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}")

if __name__ == "__main__":
    main() 